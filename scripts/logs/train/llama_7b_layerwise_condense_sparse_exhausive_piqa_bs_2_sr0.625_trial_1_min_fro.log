WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================



===================================BUG REPORT======================================================================BUG REPORT======================================================================BUG REPORT===================================



Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes



Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

===================================BUG REPORT======================================================================BUG REPORT===================================  
and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues===================================BUG REPORT=================================== 
and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issuesWelcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes




and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues 
and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================================================================================================


================================================================================
===================================BUG REPORT===================================

 
================================================================================================================================================================Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues

 
and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues================================================================================

================================================================================
bin /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/lanxiang/anaconda3/envs/llm_train did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.3/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 113
CUDA SETUP: Loading binary /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...
bin /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/lanxiang/anaconda3/envs/llm_train did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.3/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 113
CUDA SETUP: Loading binary /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...
bin /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/lanxiang/anaconda3/envs/llm_train did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.3/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 113
CUDA SETUP: Loading binary /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...
bin /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/lanxiang/anaconda3/envs/llm_train did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.3/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 113
CUDA SETUP: Loading binary /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...
bin /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/lanxiang/anaconda3/envs/llm_train did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.3/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 113
CUDA SETUP: Loading binary /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...
bin /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so
bin /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/lanxiang/anaconda3/envs/llm_train did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.3/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 113
CUDA SETUP: Loading binary /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/lanxiang/anaconda3/envs/llm_train did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.3/lib64/libcudart.so.11.0
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 113
CUDA SETUP: Loading binary /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...
bin /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/lanxiang/anaconda3/envs/llm_train did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.3/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 113
CUDA SETUP: Loading binary /home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/training_args.py:1516: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/training_args.py:1516: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/training_args.py:1516: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/training_args.py:1516: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/training_args.py:1516: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/training_args.py:1516: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/training_args.py:1516: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/training_args.py:1516: FutureWarning: using `--fsdp_transformer_layer_cls_to_wrap` is deprecated. Use fsdp_config instead 
  warnings.warn(
11/15/2023 21:15:17 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2023 21:15:17 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2023 21:15:17 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2023 21:15:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=epoch,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>],
fsdp_config={'fsdp_min_num_params': 0, 'fsdp_transformer_layer_cls_to_wrap': ['LlamaDecoderLayer'], 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=LlamaDecoderLayer,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=outputs/baffo32/decapoda-research-llama-7B-hf/piqa_bs_2/layerwise_condense_sparse_exhausive_sr0.625_trial_1_min_fro/runs/Nov15_21-15-17_l2.mit.edu,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=41.0,
optim=adamw_hf,
optim_args=None,
output_dir=outputs/baffo32/decapoda-research-llama-7B-hf/piqa_bs_2/layerwise_condense_sparse_exhausive_sr0.625_trial_1_min_fro,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=2,
per_device_train_batch_size=2,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/baffo32/decapoda-research-llama-7B-hf/piqa_bs_2/layerwise_condense_sparse_exhausive_sr0.625_trial_1_min_fro,
save_on_each_node=False,
save_safetensors=False,
save_steps=50000,
save_strategy=steps,
save_total_limit=1,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
11/15/2023 21:15:17 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2023 21:15:17 - WARNING - __main__ - Process rank: 7, device: cuda:7, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2023 21:15:17 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2023 21:15:17 - WARNING - __main__ - Process rank: 6, device: cuda:6, n_gpu: 1distributed training: True, 16-bits training: False
11/15/2023 21:15:17 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: False
[WARNING|logging.py:295] 2023-11-15 21:15:17,529 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:295] 2023-11-15 21:15:17,532 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:295] 2023-11-15 21:15:17,537 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:295] 2023-11-15 21:15:17,554 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
[WARNING|logging.py:295] 2023-11-15 21:15:17,554 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
[INFO|configuration_utils.py:712] 2023-11-15 21:15:17,557 >> loading configuration file config.json from cache at /home/lanxiang/.cache/huggingface/hub/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/config.json
[INFO|configuration_utils.py:768] 2023-11-15 21:15:17,558 >> Model config LlamaConfig {
  "_name_or_path": "baffo32/decapoda-research-llama-7B-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "bos_token_id": 0,
  "eos_token_id": 1,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 2048,
  "max_sequence_length": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "output_hidden_states": true,
  "pad_token_id": -1,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "use_cache": false,
  "vocab_size": 32000
}

[WARNING|logging.py:295] 2023-11-15 21:15:17,566 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
[INFO|tokenization_utils_base.py:1839] 2023-11-15 21:15:17,604 >> loading file tokenizer.model from cache at /home/lanxiang/.cache/huggingface/hub/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/tokenizer.model
[INFO|tokenization_utils_base.py:1839] 2023-11-15 21:15:17,604 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1839] 2023-11-15 21:15:17,604 >> loading file special_tokens_map.json from cache at /home/lanxiang/.cache/huggingface/hub/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/special_tokens_map.json
[INFO|tokenization_utils_base.py:1839] 2023-11-15 21:15:17,604 >> loading file tokenizer_config.json from cache at /home/lanxiang/.cache/huggingface/hub/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/tokenizer_config.json
[WARNING|logging.py:295] 2023-11-15 21:15:17,604 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
[INFO|modeling_utils.py:2603] 2023-11-15 21:15:17,615 >> loading weights file pytorch_model.bin from cache at /home/lanxiang/.cache/huggingface/hub/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/pytorch_model.bin.index.json
[INFO|configuration_utils.py:599] 2023-11-15 21:15:17,621 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "output_hidden_states": true,
  "pad_token_id": -1,
  "transformers_version": "4.31.0",
  "use_cache": false
}

[WARNING|logging.py:295] 2023-11-15 21:15:17,864 >> You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.94it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:12,  2.63it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:10,  3.00it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:13,  2.31it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:09,  3.38it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:09,  3.07it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:10,  2.93it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.29it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:09,  3.14it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:21,  1.50it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:26,  1.19it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.99it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:11,  2.71it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.27it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:08,  3.24it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:31,  1.01it/s]Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:08,  3.33it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.14it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:18,  1.64it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:10,  2.66it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:12,  2.29it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:24,  1.28it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:08,  2.94it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.22it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:25,  1.24it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:11,  2.46it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.22it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:25,  1.24it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:18,  1.60it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:02<00:08,  2.98it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:02<00:19,  1.51it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.39it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:10,  2.52it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:02<00:08,  2.94it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:02<00:21,  1.37it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.08it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:10,  2.57it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:17,  1.66it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:17,  1.62it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:21,  1.41it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:12,  2.26it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:07,  2.89it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:09,  2.62it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.05it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:19,  1.46it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:03<00:15,  1.80it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.12it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:02<00:19,  1.55it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:03<00:08,  2.69it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:03<00:17,  1.57it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:09,  2.66it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.06it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:14,  1.92it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:03<00:08,  2.77it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:08,  2.56it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:03<00:18,  1.51it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.04it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:17,  1.63it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:17,  1.56it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.06it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.05it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:07,  2.79it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.02it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:04<00:08,  2.25it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:04<00:17,  1.53it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:04<00:07,  2.77it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:03<00:17,  1.61it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.06it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:12,  2.06it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:04<00:16,  1.57it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:08,  2.27it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:04<00:07,  2.80it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  2.00it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.06it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:16,  1.67it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:04<00:16,  1.53it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.00it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:05<00:06,  2.89it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:07,  2.35it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.05it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:05<00:15,  1.57it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:05<00:06,  2.55it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:05<00:06,  2.88it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:11,  2.07it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:04<00:15,  1.73it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.98it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.17it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:05<00:16,  1.53it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:06,  2.63it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:05<00:05,  2.88it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:15,  1.52it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.04it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  1.98it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:09,  2.21it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:06<00:05,  2.77it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.76it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:06<00:05,  2.88it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:06<00:16,  1.49it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:06<00:04,  2.89it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:10,  2.08it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:06<00:05,  2.94it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.30it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.03it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:06<00:14,  1.54it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:13,  1.78it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:04,  2.80it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:06<00:04,  2.90it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:08,  2.35it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:06<00:14,  1.61it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.04it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:07<00:04,  2.82it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:07<00:04,  2.88it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:09,  1.81it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:06<00:13,  1.71it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.36it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:07<00:14,  1.51it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:07<00:12,  1.75it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:09,  1.99it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:07<00:04,  2.90it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:08<00:04,  2.67it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:07,  2.30it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:07<00:03,  2.80it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:13,  1.68it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:07<00:11,  1.76it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:08<00:03,  2.61it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:09,  1.64it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:07<00:14,  1.46it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:06,  2.38it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:08<00:03,  2.87it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.69it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:08<00:03,  2.74it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:07<00:12,  1.75it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:08<00:10,  1.82it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:08<00:03,  2.97it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:06,  2.41it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:09,  1.62it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:08<00:13,  1.46it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:10,  1.67it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:10,  1.82it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:05,  2.46it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:08<00:10,  1.88it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:08,  1.61it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.49it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:09<00:12,  1.51it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:09<00:09,  1.94it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:08<00:10,  1.88it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.64it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:04,  2.42it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:08,  1.61it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:08,  2.01it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:09,  1.88it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:09<00:11,  1.53it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.47it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:09,  1.62it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:10<00:07,  2.09it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:09,  1.87it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:07,  1.61it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.47it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:07,  1.05it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:06,  1.16it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:10<00:11,  1.53it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:07,  2.14it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:08,  1.56it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:11<00:05,  1.30it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:10<00:03,  2.37it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:11<00:05,  1.38it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:08,  1.80it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:07,  1.56it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:06,  2.17it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:11<00:10,  1.53it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:03,  1.52it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:10<00:03,  2.42it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:03,  1.56it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:08,  1.53it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:05,  2.20it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:12<00:02,  1.80it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.78it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:12<00:06,  1.56it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:11<00:02,  2.39it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:11<00:09,  1.53it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:06,  1.74it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:12<00:03,  1.65it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:05,  2.22it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.45it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:07,  1.77it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:05,  2.02it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.56it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:04,  2.25it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:12<00:04,  2.28it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:12<00:09,  1.53it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:12<00:02,  2.48it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.74it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:03,  2.50it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:12<00:04,  2.23it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:13<00:04,  1.61it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:03,  1.23it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.30it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:13<00:03,  2.66it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:12<00:08,  1.56it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:03,  2.25it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:03,  1.18it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:12<00:06,  1.72it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:13<00:02,  1.48it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:13<00:01,  2.32it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:04,  1.58it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:13<00:02,  2.34it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:01,  1.72it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:13<00:03,  2.23it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:13<00:07,  1.54it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:13<00:00,  2.34it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.70it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:02,  1.24it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:14<00:00,  1.93it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:03,  2.24it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.55it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:14<00:02,  2.02it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:13<00:00,  2.34it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:14<00:07,  1.57it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.70it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  1.98it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.18it/s]
[ERROR|tokenization_utils_base.py:1056] 2023-11-15 21:16:02,117 >> Using pad_token, but it is not set yet.
Loading checkpoint shards:  82%|████████▏ | 27/33 [00:14<00:02,  2.20it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:01,  1.30it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:14<00:00,  2.16it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:14<00:00,  2.28it/s]
[ERROR|tokenization_utils_base.py:1056] 2023-11-15 21:16:02,399 >> Using pad_token, but it is not set yet.
Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  1.81it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:15<00:03,  1.50it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:14<00:06,  1.58it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:14<00:02,  2.23it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.70it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  1.36it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:15<00:01,  2.22it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.80it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:15<00:05,  1.60it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:15<00:02,  1.63it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.43it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:15<00:01,  2.25it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.05it/s]
[ERROR|tokenization_utils_base.py:1056] 2023-11-15 21:16:03,567 >> Using pad_token, but it is not set yet.
Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:03,  1.91it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:16<00:04,  1.62it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:16<00:01,  1.59it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:16<00:00,  2.24it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:02,  1.43it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:02,  2.00it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:16<00:00,  2.22it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:15<00:02,  1.99it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.58it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:16<00:04,  1.45it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.40it/s]11/15/2023 21:16:04 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 894.18it/s]
Loading checkpoint shards: 100%|██████████| 33/33 [00:17<00:00,  2.09it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:17<00:00,  1.90it/s]
[ERROR|tokenization_utils_base.py:1056] 2023-11-15 21:16:05,057 >> Using pad_token, but it is not set yet.
Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.85it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:17<00:00,  1.57it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:17<00:04,  1.48it/s]11/15/2023 21:16:05 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1041.89it/s]
Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.41it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.81it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:18<00:03,  1.45it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.71it/s]
[ERROR|tokenization_utils_base.py:1056] 2023-11-15 21:16:06,056 >> Using pad_token, but it is not set yet.
Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.78it/s]
[INFO|modeling_utils.py:3329] 2023-11-15 21:16:06,057 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:3337] 2023-11-15 21:16:06,057 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at baffo32/decapoda-research-llama-7B-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:561] 2023-11-15 21:16:06,095 >> loading configuration file generation_config.json from cache at /home/lanxiang/.cache/huggingface/hub/models--baffo32--decapoda-research-llama-7B-hf/snapshots/aa18b48a1330572a6dd5f5d5619ed19838ca285c/generation_config.json
[INFO|configuration_utils.py:599] 2023-11-15 21:16:06,095 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0,
  "transformers_version": "4.31.0"
}

11/15/2023 21:16:06 - INFO - __main__ - LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (1): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (2): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (3): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (4): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (5): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (6): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (7): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (8): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (9): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (10): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (11): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (12): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (13): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (14): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (15): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (16): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (17): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (18): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (19): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (20): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (21): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (22): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (23): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (24): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (25): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (26): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (27): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (28): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (29): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (30): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
      (31): LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
11/15/2023 21:16:06 - INFO - __main__ - model.embed_tokens.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.0.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.0.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.0.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.0.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.0.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.0.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.0.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.0.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.0.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.1.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.1.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.1.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.1.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.1.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.1.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.1.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.1.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.1.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.2.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.2.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.2.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.2.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.2.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.2.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.2.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.2.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.2.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.3.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.3.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.3.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.3.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.3.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.3.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.3.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.3.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.3.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.4.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.4.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.4.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.4.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.4.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.4.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.4.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.4.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.4.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.5.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.5.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.5.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.5.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.5.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.5.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.5.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.5.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.5.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.6.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.6.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.6.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.6.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.6.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.6.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.6.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.6.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.6.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.7.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.7.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.7.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.7.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.7.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.7.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.7.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.7.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.7.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.8.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.8.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.8.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.8.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.8.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.8.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.8.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.8.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.8.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.9.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.9.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.9.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.9.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.9.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.9.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.9.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.9.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.9.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.10.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.10.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.10.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.10.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.10.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.10.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.10.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.10.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.10.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.11.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.11.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.11.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.11.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.11.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.11.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.11.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.11.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.11.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.12.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.12.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.12.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.12.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.12.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.12.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.12.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.12.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.12.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.13.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.13.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.13.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.13.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.13.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.13.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.13.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.13.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.13.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.14.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.14.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.14.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.14.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.14.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.14.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.14.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.14.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.14.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.15.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.15.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.15.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.15.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.15.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.15.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.15.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.15.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.15.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.16.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.16.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.16.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.16.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.16.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.16.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.16.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.16.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.16.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.17.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.17.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.17.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.17.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.17.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.17.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.17.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.17.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.17.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.18.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.18.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.18.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.18.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.18.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.18.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.18.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.18.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.18.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.19.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.19.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.19.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.19.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.19.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.19.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.19.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.19.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.19.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.20.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.20.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.20.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.20.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.20.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.20.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.20.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.20.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.20.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.21.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.21.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.21.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.21.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.21.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.21.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.21.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.21.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.21.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.22.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.22.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.22.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.22.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.22.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.22.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.22.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.22.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.22.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.23.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.23.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.23.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.23.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.23.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.23.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.23.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.23.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.23.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.24.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.24.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.24.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.24.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.24.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.24.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.24.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.24.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.24.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.25.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.25.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.25.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.25.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.25.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.25.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.25.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.25.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.25.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.26.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.26.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.26.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.26.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.26.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.26.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.26.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.26.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.26.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.27.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.27.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.27.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.27.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.27.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.27.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.27.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.27.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.27.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.28.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.28.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.28.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.28.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.28.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.28.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.28.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.28.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.28.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.29.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.29.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.29.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.29.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.29.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.29.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.29.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.29.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.29.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.30.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.30.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.30.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.30.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.30.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.30.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.30.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.30.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.30.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.31.self_attn.q_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.31.self_attn.k_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.31.self_attn.v_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.31.self_attn.o_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.31.mlp.gate_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.31.mlp.up_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.31.mlp.down_proj.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.31.input_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.layers.31.post_attention_layernorm.weight
11/15/2023 21:16:06 - INFO - __main__ - model.norm.weight
11/15/2023 21:16:06 - INFO - __main__ - lm_head.weight
[ERROR|tokenization_utils_base.py:1056] 2023-11-15 21:16:06,106 >> Using pad_token, but it is not set yet.
[INFO|tokenization_utils_base.py:921] 2023-11-15 21:16:06,107 >> Assigning [PAD] to the pad_token key of the tokenizer
[INFO|tokenization_utils.py:426] 2023-11-15 21:16:06,107 >> Adding [PAD] to the vocabulary
11/15/2023 21:16:06 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1058.37it/s]
Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.76it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:18<00:02,  1.48it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.72it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:19<00:01,  1.50it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.64it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.74it/s]
[ERROR|tokenization_utils_base.py:1056] 2023-11-15 21:16:07,683 >> Using pad_token, but it is not set yet.
11/15/2023 21:16:07 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1244.60it/s]
Loading checkpoint shards:  94%|█████████▍| 31/33 [00:20<00:01,  1.53it/s][INFO|tokenization_utils_base.py:921] 2023-11-15 21:16:08,428 >> Assigning </s> to the eos_token key of the tokenizer
[INFO|tokenization_utils_base.py:921] 2023-11-15 21:16:08,428 >> Assigning </s> to the bos_token key of the tokenizer
[INFO|tokenization_utils_base.py:921] 2023-11-15 21:16:08,428 >> Assigning </s> to the unk_token key of the tokenizer
Loading checkpoint shards:  97%|█████████▋| 32/33 [00:20<00:00,  1.56it/s]11/15/2023 21:16:08 - INFO - datasets.builder - No config specified, defaulting to the single config: piqa/plain_text
11/15/2023 21:16:08 - INFO - datasets.info - Loading Dataset Infos from /home/lanxiang/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:16:08 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/15/2023 21:16:08 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:16:08 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1272.67it/s]
11/15/2023 21:16:08 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
11/15/2023 21:16:08 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1242.02it/s]
11/15/2023 21:16:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e8e64964e9fa00a2.arrow
11/15/2023 21:16:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e1d1ca6f75758c3f.arrow
11/15/2023 21:16:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-50d3711a989b366c.arrow
Loading checkpoint shards: 100%|██████████| 33/33 [00:21<00:00,  1.51it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:21<00:00,  1.53it/s]
[ERROR|tokenization_utils_base.py:1056] 2023-11-15 21:16:09,233 >> Using pad_token, but it is not set yet.
11/15/2023 21:16:10 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 889.31it/s]
11/15/2023 21:16:12 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1142.03it/s]
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e8e64964e9fa00a2.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e8e64964e9fa00a2.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e8e64964e9fa00a2.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e8e64964e9fa00a2.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e8e64964e9fa00a2.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e8e64964e9fa00a2.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e8e64964e9fa00a2.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e1d1ca6f75758c3f.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e1d1ca6f75758c3f.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e1d1ca6f75758c3f.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e1d1ca6f75758c3f.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e1d1ca6f75758c3f.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e1d1ca6f75758c3f.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e1d1ca6f75758c3f.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-50d3711a989b366c.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-50d3711a989b366c.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-50d3711a989b366c.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-50d3711a989b366c.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-50d3711a989b366c.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-50d3711a989b366c.arrow
11/15/2023 21:16:13 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-50d3711a989b366c.arrow
11/15/2023 21:16:14 - WARNING - data - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.
11/15/2023 21:16:14 - WARNING - data - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.
11/15/2023 21:16:14 - WARNING - data - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.
11/15/2023 21:16:14 - WARNING - data - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.
11/15/2023 21:16:14 - WARNING - data - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.
11/15/2023 21:16:14 - WARNING - data - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e5450fa1be09ea0c.arrow
11/15/2023 21:16:14 - WARNING - data - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cb9227a1b842937a.arrow
11/15/2023 21:16:14 - WARNING - data - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --block_size xxx.
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cba32b220b82ad80.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e5450fa1be09ea0c.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e5450fa1be09ea0c.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e5450fa1be09ea0c.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e5450fa1be09ea0c.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e5450fa1be09ea0c.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e5450fa1be09ea0c.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-e5450fa1be09ea0c.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cb9227a1b842937a.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cb9227a1b842937a.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cb9227a1b842937a.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cb9227a1b842937a.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cb9227a1b842937a.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cb9227a1b842937a.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cb9227a1b842937a.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cba32b220b82ad80.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cba32b220b82ad80.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cba32b220b82ad80.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cba32b220b82ad80.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cba32b220b82ad80.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cba32b220b82ad80.arrow
11/15/2023 21:16:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011/cache-cba32b220b82ad80.arrow
11/15/2023 21:16:14 - INFO - __main__ - calculating zero-shot accuracy...
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 4096)
        (layers): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (4): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (5): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (6): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (7): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (8): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (9): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (10): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (11): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (12): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (13): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (14): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (15): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (16): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (17): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (18): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (19): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (20): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (21): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (22): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (23): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (24): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (25): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (26): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (27): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (28): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (29): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (30): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (31): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
    )
  )
)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 4096)
        (layers): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (4): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (5): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (6): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (7): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (8): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (9): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (10): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (11): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (12): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (13): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (14): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (15): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (16): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (17): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (18): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (19): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (20): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (21): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (22): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (23): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (24): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (25): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (26): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (27): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (28): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (29): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (30): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (31): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
    )
  )
)
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 4096)
        (layers): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (4): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (5): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (6): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (7): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (8): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (9): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (10): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (11): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (12): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (13): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (14): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (15): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (16): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (17): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (18): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (19): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (20): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (21): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (22): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (23): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (24): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (25): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (26): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (27): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (28): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (29): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (30): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (31): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
    )
  )
)
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 4096)
        (layers): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (4): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (5): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (6): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (7): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (8): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (9): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (10): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (11): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (12): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (13): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (14): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (15): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (16): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (17): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (18): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (19): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (20): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (21): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (22): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (23): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (24): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (25): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (26): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (27): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (28): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (29): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (30): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (31): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
    )
  )
)
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 4096)
        (layers): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (4): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (5): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (6): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (7): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (8): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (9): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (10): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (11): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (12): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (13): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (14): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (15): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (16): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (17): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (18): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (19): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (20): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (21): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (22): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (23): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (24): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (25): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (26): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (27): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (28): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (29): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (30): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (31): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
    )
  )
)
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 4096)
        (layers): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (4): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (5): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (6): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (7): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (8): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (9): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (10): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (11): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (12): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (13): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (14): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (15): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (16): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (17): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (18): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (19): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (20): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (21): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (22): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (23): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (24): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (25): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (26): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (27): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (28): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (29): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (30): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (31): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
    )
  )
)
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
11/15/2023 21:16:26 - INFO - __main__ -  ------------------- zero-shot accuracy reported above! -------------------
static mask string matches:
[]
creating directory...
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 16113
  Num Epochs = 41
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 41328
  Number of trainable parameters = 842302976
  0%|          | 0/41328 [00:00<?, ?it/s]layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
layer frozen? False
to train:
['_fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.0._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.1._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.2._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.3._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.4._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.5._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.6._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.7._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.8._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.9._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.10._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.11._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.12._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.13._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.14._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.15._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.16._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.17._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.18._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.19._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.20._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.21._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.22._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.23._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.24._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.25._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.26._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.27._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.28._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.29._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.30._fsdp_wrapped_module.flat_param', '_fsdp_wrapped_module._fpw_module.model.layers.31._fsdp_wrapped_module.flat_param']
setting up static policy for sparse update (only train important layers):
initiating evaluation loop...
evaluation loop.
***** Running Evaluation *****
  Num examples = 1838
  Batch size = 2
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 4096)
        (layers): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (4): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (5): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (6): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (7): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (8): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (9): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (10): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (11): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (12): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (13): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (14): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (15): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (16): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (17): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (18): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (19): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (20): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (21): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (22): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (23): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (24): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (25): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (26): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (27): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (28): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (29): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (30): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (31): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
    )
  )
)
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
FullyShardedDataParallel(
  (_fsdp_wrapped_module): FlattenParamsWrapper(
    (_fpw_module): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(32001, 4096)
        (layers): ModuleList(
          (0): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (1): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (2): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (3): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (4): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (5): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (6): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (7): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (8): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (9): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (10): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (11): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (12): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (13): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (14): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (15): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (16): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (17): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (18): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (19): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (20): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (21): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (22): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (23): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (24): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (25): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (26): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (27): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (28): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (29): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (30): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
          (31): FullyShardedDataParallel(
            (_fsdp_wrapped_module): FlattenParamsWrapper(
              (_fpw_module): LlamaDecoderLayer(
                (self_attn): LlamaAttention(
                  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
                  (rotary_emb): LlamaRotaryEmbedding()
                )
                (mlp): LlamaMLP(
                  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
                  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
                  (act_fn): SiLUActivation()
                )
                (input_layernorm): LlamaRMSNorm()
                (post_attention_layernorm): LlamaRMSNorm()
              )
            )
          )
        )
        (norm): LlamaRMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)
    )
  )
)
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(

  0%|          | 0/115 [00:00<?, ?it/s][A
  2%|▏         | 2/115 [00:01<01:48,  1.04it/s][A
  3%|▎         | 3/115 [00:03<02:33,  1.37s/it][A
  3%|▎         | 4/115 [00:05<02:53,  1.56s/it][A
  4%|▍         | 5/115 [00:07<03:04,  1.67s/it][A
  5%|▌         | 6/115 [00:09<03:09,  1.73s/it][A
  6%|▌         | 7/115 [00:11<03:11,  1.77s/it][A
  7%|▋         | 8/115 [00:13<03:12,  1.80s/it][A
  8%|▊         | 9/115 [00:15<03:12,  1.82s/it][A
  9%|▊         | 10/115 [00:16<03:13,  1.84s/it][A
 10%|▉         | 11/115 [00:18<03:13,  1.86s/it][A
 10%|█         | 12/115 [00:20<03:14,  1.89s/it][A
 11%|█▏        | 13/115 [00:22<03:12,  1.89s/it][A
 12%|█▏        | 14/115 [00:24<03:10,  1.89s/it][A
 13%|█▎        | 15/115 [00:26<03:09,  1.90s/it][A
 14%|█▍        | 16/115 [00:28<03:06,  1.89s/it][A
 15%|█▍        | 17/115 [00:30<03:04,  1.88s/it][A
 16%|█▌        | 18/115 [00:32<03:01,  1.87s/it][A
 17%|█▋        | 19/115 [00:33<02:58,  1.86s/it][Aall layers:
dict_keys(['0 attn', '0 mlp', '1 attn', '1 mlp', '2 attn', '2 mlp', '3 attn', '3 mlp', '4 attn', '4 mlp', '5 attn', '5 mlp', '6 attn', '6 mlp', '7 attn', '7 mlp', '8 attn', '8 mlp', '9 attn', '9 mlp', '10 attn', '10 mlp', '11 attn', '11 mlp', '12 attn', '12 mlp', '13 attn', '13 mlp', '14 attn', '14 mlp', '15 attn', '15 mlp', '16 attn', '16 mlp', '17 attn', '17 mlp', '18 attn', '18 mlp', '19 attn', '19 mlp', '20 attn', '20 mlp', '21 attn', '21 mlp', '22 attn', '22 mlp', '23 attn', '23 mlp', '24 attn', '24 mlp', '25 attn', '25 mlp', '26 attn', '26 mlp', '27 attn', '27 mlp', '28 attn', '28 mlp', '29 attn', '29 mlp', '30 attn', '30 mlp', '31 attn', '31 mlp'])
statistics for activations for 0 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.00014463132538367063
std: 0.05724722892045975
kurtosis: 259.88539179772647
l-1 norm: 178870.296875
l-2 norm: 133.5002899169922
Forbenius norm: 133.5002899169922
statistics for activations for 0 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.0008607874042354524
std: 0.041194625198841095
kurtosis: 72.3777720394807
l-1 norm: 76075.84375
l-2 norm: 67.95451354980469
Forbenius norm: 67.95451354980469
statistics for activations for 1 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0009553672862239182
std: 0.031680021435022354
kurtosis: 333.1443875277886
l-1 norm: 88914.421875
l-2 norm: 73.90388488769531
Forbenius norm: 73.90388488769531
statistics for activations for 1 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.000517992302775383
std: 0.05611943081021309
kurtosis: 2471.068812514728
l-1 norm: 65980.7109375
l-2 norm: 92.54953002929688
Forbenius norm: 92.54953002929688
statistics for activations for 2 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0006417283439077437
std: 0.04944295436143875
kurtosis: 1353.5010273467162
l-1 norm: 97689.484375
l-2 norm: 115.25607299804688
Forbenius norm: 115.25607299804688
statistics for activations for 2 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.055980004370212555
std: 5.521092891693115
kurtosis: 67678.1622899009
l-1 norm: 400774.4375
l-2 norm: 9105.2880859375
Forbenius norm: 9105.2880859375
statistics for activations for 3 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: 0.00013744799070991576
std: 0.025670887902379036
kurtosis: 57.83384787611856
l-1 norm: 92597.7109375
l-2 norm: 59.86937713623047
Forbenius norm: 59.86937713623047
statistics for activations for 3 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -4.6577864850405604e-05
std: 0.028967196121811867
kurtosis: 237.37633312833375
l-1 norm: 51812.93359375
l-2 norm: 47.773197174072266
Forbenius norm: 47.773197174072266
statistics for activations for 4 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: 0.0004242251452524215
std: 0.1351383626461029
kurtosis: 110.41460680324872
l-1 norm: 467879.96875
l-2 norm: 315.0846862792969
Forbenius norm: 315.0846862792969
statistics for activations for 4 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.003239960176870227
std: 0.3193613588809967
kurtosis: 4859.211262861251
l-1 norm: 271108.5
l-2 norm: 526.5986938476562
Forbenius norm: 526.5986938476562
statistics for activations for 5 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: 6.17831974523142e-05
std: 0.04715745896100998
kurtosis: 44.746094945551604
l-1 norm: 175073.0
l-2 norm: 109.98509216308594
Forbenius norm: 109.98509216308594
statistics for activations for 5 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -7.320363692997489e-06
std: 0.03153077885508537
kurtosis: 14.22337446889011
l-1 norm: 50549.08203125
l-2 norm: 51.996402740478516
Forbenius norm: 51.996402740478516
statistics for activations for 6 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0006642249063588679
std: 0.03652967885136604
kurtosis: 445.71587465979104
l-1 norm: 133352.328125
l-2 norm: 85.21037292480469
Forbenius norm: 85.21037292480469
statistics for activations for 6 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.00028509474941529334
std: 0.03125027194619179
kurtosis: 11.886790343234793
l-1 norm: 49092.875
l-2 norm: 51.53493881225586
Forbenius norm: 51.53493881225586
statistics for activations for 7 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: 3.080868918914348e-05
std: 0.04784376546740532
kurtosis: 14.30666389306215
l-1 norm: 190838.96875
l-2 norm: 111.59171295166016
Forbenius norm: 111.59171295166016
statistics for activations for 7 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.000449389306595549
std: 0.03667335584759712
kurtosis: 13.515235380511967
l-1 norm: 58955.9609375
l-2 norm: 60.48255157470703
Forbenius norm: 60.48255157470703
statistics for activations for 8 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0009345963480882347
std: 0.03769829496741295
kurtosis: 9.158547171628273
l-1 norm: 156621.40625
l-2 norm: 87.95255279541016
Forbenius norm: 87.95255279541016
statistics for activations for 8 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -1.8387338059255853e-05
std: 0.04338063672184944
kurtosis: 13.86142483949353
l-1 norm: 69942.765625
l-2 norm: 71.53959655761719
Forbenius norm: 71.53959655761719
statistics for activations for 9 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0008167902124114335
std: 0.042062871158123016
kurtosis: 5.44277713041013
l-1 norm: 176059.1875
l-2 norm: 98.12311553955078
Forbenius norm: 98.12311553955078
statistics for activations for 9 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.00014866235142108053
std: 0.04414883628487587
kurtosis: 12.98818076010589
l-1 norm: 74397.1484375
l-2 norm: 72.80823516845703
Forbenius norm: 72.80823516845703
statistics for activations for 10 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0005406095297075808
std: 0.05038180202245712
kurtosis: 52.147175929486075
l-1 norm: 202055.328125
l-2 norm: 117.51419830322266
Forbenius norm: 117.51419830322266
statistics for activations for 10 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -5.108273035148159e-05
std: 0.049225084483623505
kurtosis: 8.62666970588379
l-1 norm: 83587.6875
l-2 norm: 81.17829895019531
Forbenius norm: 81.17829895019531
statistics for activations for 11 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.00047948930296115577
std: 0.053922660648822784
kurtosis: 24.36308794419846
l-1 norm: 223884.0
l-2 norm: 125.77339935302734
Forbenius norm: 125.77339935302734
statistics for activations for 11 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.00030097784474492073
std: 0.05725464969873428
kurtosis: 39.16324676551839
l-1 norm: 104616.4140625
l-2 norm: 94.42681884765625
Forbenius norm: 94.42681884765625
statistics for activations for 12 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0005998431006446481
std: 0.054498132318258286
kurtosis: 20.3101339478206
l-1 norm: 226208.21875
l-2 norm: 127.11871337890625
Forbenius norm: 127.11871337890625
statistics for activations for 12 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.0005703296628780663
std: 0.07584477961063385
kurtosis: 175.10412206188496
l-1 norm: 138685.4375
l-2 norm: 125.08775329589844
Forbenius norm: 125.08775329589844
statistics for activations for 13 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0012572702253237367
std: 0.06142476946115494
kurtosis: 26.574058421265036
l-1 norm: 255102.078125
l-2 norm: 143.29580688476562
Forbenius norm: 143.29580688476562
statistics for activations for 13 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -5.2939583838451654e-05
std: 0.06483831256628036
kurtosis: 18.084747941295028
l-1 norm: 117189.78125
l-2 norm: 106.93093872070312
Forbenius norm: 106.93093872070312
statistics for activations for 14 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.00018257502233609557
std: 0.06212972104549408
kurtosis: 168.07320121193584
l-1 norm: 239996.21875
l-2 norm: 144.90467834472656
Forbenius norm: 144.90467834472656
statistics for activations for 14 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.00030839871033094823
std: 0.07633797824382782
kurtosis: 88.86437874609094
l-1 norm: 134356.78125
l-2 norm: 125.89631652832031
Forbenius norm: 125.89631652832031
statistics for activations for 15 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: 0.0004770198429469019
std: 0.06465581804513931
kurtosis: 3.0566137714965738
l-1 norm: 274120.5625
l-2 norm: 150.80377197265625
Forbenius norm: 150.80377197265625
statistics for activations for 15 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.0003348559548612684
std: 0.07477346807718277
kurtosis: 13.312033711268157
l-1 norm: 134336.28125
l-2 norm: 123.31524658203125
Forbenius norm: 123.31524658203125
statistics for activations for 16 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: 0.0003646844415925443
std: 0.07500095665454865
kurtosis: 308.0052431604418
l-1 norm: 275887.28125
l-2 norm: 174.91522216796875
Forbenius norm: 174.91522216796875
statistics for activations for 16 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.0005209266673773527
std: 0.1247483640909195
kurtosis: 88.6188477381058
l-1 norm: 230576.890625
l-2 norm: 205.73936462402344
Forbenius norm: 205.73936462402344
statistics for activations for 17 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: 0.0007273015799000859
std: 0.07060693204402924
kurtosis: 4.376020567624288
l-1 norm: 298650.125
l-2 norm: 164.6888427734375
Forbenius norm: 164.6888427734375
statistics for activations for 17 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.0027646746020764112
std: 0.2058177888393402
kurtosis: 46.72962076502161
l-1 norm: 389379.125
l-2 norm: 339.4767761230469
Forbenius norm: 339.4767761230469
statistics for activations for 18 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0002296401362400502
std: 0.06474044919013977
kurtosis: 90.54158734899792
l-1 norm: 259126.84375
l-2 norm: 150.9941864013672
Forbenius norm: 150.9941864013672
statistics for activations for 18 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.001041275798343122
std: 0.11402129381895065
kurtosis: 97.00950015516824
l-1 norm: 209699.609375
l-2 norm: 188.0550537109375
Forbenius norm: 188.0550537109375
statistics for activations for 19 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: 0.0008846379350870848
std: 0.07191663235425949
kurtosis: 76.75219452203518
l-1 norm: 284941.53125
l-2 norm: 167.74098205566406
Forbenius norm: 167.74098205566406
statistics for activations for 19 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.0002805782714858651
std: 0.10981975495815277
kurtosis: 49.54650404067241
l-1 norm: 203097.65625
l-2 norm: 181.11749267578125
Forbenius norm: 181.11749267578125
statistics for activations for 20 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.000990330590866506
std: 0.07492893189191818
kurtosis: 133.92002604514389
l-1 norm: 288103.84375
l-2 norm: 174.76626586914062
Forbenius norm: 174.76626586914062
statistics for activations for 20 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.002810019999742508
std: 0.1447562724351883
kurtosis: 36.9186887351212
l-1 norm: 289959.84375
l-2 norm: 238.78607177734375
Forbenius norm: 238.78607177734375
statistics for activations for 21 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0015696187037974596
std: 0.1354091763496399
kurtosis: 2500.716788882592
l-1 norm: 281963.53125
l-2 norm: 315.6474914550781
Forbenius norm: 315.6474914550781
statistics for activations for 21 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.000139730516821146
std: 0.08620389550924301
kurtosis: 42.65978294528761
l-1 norm: 149135.703125
l-2 norm: 142.1644287109375
Forbenius norm: 142.1644287109375
statistics for activations for 22 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0002471479238010943
std: 0.09079858660697937
kurtosis: 1587.120781990049
l-1 norm: 240384.84375
l-2 norm: 211.7041473388672
Forbenius norm: 211.7041473388672
statistics for activations for 22 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.0006136485608294606
std: 0.09694000333547592
kurtosis: 33.871218546702664
l-1 norm: 186494.125
l-2 norm: 159.8805389404297
Forbenius norm: 159.8805389404297
statistics for activations for 23 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0009951848769560456
std: 0.18041777610778809
kurtosis: 3029.9231873325243
l-1 norm: 389975.40625
l-2 norm: 420.63427734375
Forbenius norm: 420.63427734375
statistics for activations for 23 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.0008810568251647055
std: 0.07126286625862122
kurtosis: 27.706756408289667
l-1 norm: 123905.265625
l-2 norm: 117.5331039428711
Forbenius norm: 117.5331039428711
statistics for activations for 24 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.001067395438440144
std: 0.08702490478754044
kurtosis: 1398.1303795998979
l-1 norm: 236403.203125
l-2 norm: 202.92715454101562
Forbenius norm: 202.92715454101562
statistics for activations for 24 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -5.5215095926541835e-05
std: 0.08267518132925034
kurtosis: 370.8426850217793
l-1 norm: 134894.0
l-2 norm: 136.34300231933594
Forbenius norm: 136.34300231933594
statistics for activations for 25 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: 2.5031460609170608e-05
std: 0.08340828120708466
kurtosis: 1252.6546847017921
l-1 norm: 237268.234375
l-2 norm: 194.4773712158203
Forbenius norm: 194.4773712158203
statistics for activations for 25 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.00010867630044231191
std: 0.0720374584197998
kurtosis: 8.597757548167074
l-1 norm: 126883.21875
l-2 norm: 118.80149841308594
Forbenius norm: 118.80149841308594
statistics for activations for 26 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0007051306893117726
std: 0.08641230314970016
kurtosis: 1109.8170423523159
l-1 norm: 252298.875
l-2 norm: 201.4883575439453
Forbenius norm: 201.4883575439453
statistics for activations for 26 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.0003400378627702594
std: 0.07715437561273575
kurtosis: 9.13138980599569
l-1 norm: 137624.0
l-2 norm: 127.2415542602539
Forbenius norm: 127.2415542602539
statistics for activations for 27 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0027132085524499416
std: 0.15103605389595032
kurtosis: 2991.2675420941
l-1 norm: 264099.84375
l-2 norm: 352.0369873046875
Forbenius norm: 352.0369873046875
statistics for activations for 27 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.00015318469377234578
std: 0.09545113891363144
kurtosis: 7.427175903150937
l-1 norm: 178233.75
l-2 norm: 157.41717529296875
Forbenius norm: 157.41717529296875
statistics for activations for 28 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.0005547254113480449
std: 0.14585399627685547
kurtosis: 610.1914823921488
l-1 norm: 488520.6875
l-2 norm: 340.117431640625
Forbenius norm: 340.117431640625
statistics for activations for 28 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.014921998605132103
std: 1.0010837316513062
kurtosis: 88.00315861795858
l-1 norm: 1653176.5
l-2 norm: 1651.041015625
Forbenius norm: 1651.041015625
statistics for activations for 29 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: 0.000278222345514223
std: 0.14130166172981262
kurtosis: 1390.989116958824
l-1 norm: 393943.0
l-2 norm: 329.47283935546875
Forbenius norm: 329.47283935546875
statistics for activations for 29 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.0025945145171135664
std: 0.17576462030410767
kurtosis: 5094.574119914037
l-1 norm: 325698.0
l-2 norm: 289.9035949707031
Forbenius norm: 289.9035949707031
statistics for activations for 30 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.002270880388095975
std: 0.14604206383228302
kurtosis: 851.3129694129565
l-1 norm: 416044.34375
l-2 norm: 340.55841064453125
Forbenius norm: 340.55841064453125
statistics for activations for 30 mlp is shown below...
shape: torch.Size([664, 4096])
mean: -0.034602586179971695
std: 4.479946613311768
kurtosis: 65984.00562273938
l-1 norm: 775587.25
l-2 norm: 7386.60009765625
Forbenius norm: 7386.60009765625
statistics for activations for 31 attn is shown below...
shape: torch.Size([2, 664, 4096])
mean: -0.028101634234189987
std: 2.17936372756958
kurtosis: 9001.198309422904
l-1 norm: 1208335.625
l-2 norm: 5080.79833984375
Forbenius norm: 5080.79833984375
statistics for activations for 31 mlp is shown below...
shape: torch.Size([664, 4096])
mean: 0.009385445155203342
std: 1.0872029066085815
kurtosis: 436.1333106872215
l-1 norm: 1646553.875
l-2 norm: 1793.0244140625
Forbenius norm: 1793.0244140625
all layer mean: [tensor(-0.0001), tensor(0.0009), tensor(-0.0010), tensor(0.0005), tensor(-0.0006), tensor(0.0560), tensor(0.0001), tensor(-4.6578e-05), tensor(0.0004), tensor(0.0032), tensor(6.1783e-05), tensor(-7.3204e-06), tensor(-0.0007), tensor(0.0003), tensor(3.0809e-05), tensor(0.0004), tensor(-0.0009), tensor(-1.8387e-05), tensor(-0.0008), tensor(0.0001), tensor(-0.0005), tensor(-5.1083e-05), tensor(-0.0005), tensor(-0.0003), tensor(-0.0006), tensor(0.0006), tensor(-0.0013), tensor(-5.2940e-05), tensor(-0.0002), tensor(0.0003), tensor(0.0005), tensor(0.0003), tensor(0.0004), tensor(-0.0005), tensor(0.0007), tensor(-0.0028), tensor(-0.0002), tensor(-0.0010), tensor(0.0009), tensor(0.0003), tensor(-0.0010), tensor(-0.0028), tensor(-0.0016), tensor(-0.0001), tensor(-0.0002), tensor(0.0006), tensor(-0.0010), tensor(-0.0009), tensor(-0.0011), tensor(-5.5215e-05), tensor(2.5031e-05), tensor(0.0001), tensor(-0.0007), tensor(-0.0003), tensor(-0.0027), tensor(-0.0002), tensor(-0.0006), tensor(0.0149), tensor(0.0003), tensor(-0.0026), tensor(-0.0023), tensor(-0.0346), tensor(-0.0281), tensor(0.0094)]
all layer std: [tensor(0.0572), tensor(0.0412), tensor(0.0317), tensor(0.0561), tensor(0.0494), tensor(5.5211), tensor(0.0257), tensor(0.0290), tensor(0.1351), tensor(0.3194), tensor(0.0472), tensor(0.0315), tensor(0.0365), tensor(0.0313), tensor(0.0478), tensor(0.0367), tensor(0.0377), tensor(0.0434), tensor(0.0421), tensor(0.0441), tensor(0.0504), tensor(0.0492), tensor(0.0539), tensor(0.0573), tensor(0.0545), tensor(0.0758), tensor(0.0614), tensor(0.0648), tensor(0.0621), tensor(0.0763), tensor(0.0647), tensor(0.0748), tensor(0.0750), tensor(0.1247), tensor(0.0706), tensor(0.2058), tensor(0.0647), tensor(0.1140), tensor(0.0719), tensor(0.1098), tensor(0.0749), tensor(0.1448), tensor(0.1354), tensor(0.0862), tensor(0.0908), tensor(0.0969), tensor(0.1804), tensor(0.0713), tensor(0.0870), tensor(0.0827), tensor(0.0834), tensor(0.0720), tensor(0.0864), tensor(0.0772), tensor(0.1510), tensor(0.0955), tensor(0.1459), tensor(1.0011), tensor(0.1413), tensor(0.1758), tensor(0.1460), tensor(4.4799), tensor(2.1794), tensor(1.0872)]
all layer Frobenius norms: [133.5002899169922, 67.95451354980469, 73.90388488769531, 92.54953002929688, 115.25607299804688, 9105.2880859375, 59.86937713623047, 47.773197174072266, 315.0846862792969, 526.5986938476562, 109.98509216308594, 51.996402740478516, 85.21037292480469, 51.53493881225586, 111.59171295166016, 60.48255157470703, 87.95255279541016, 71.53959655761719, 98.12311553955078, 72.80823516845703, 117.51419830322266, 81.17829895019531, 125.77339935302734, 94.42681884765625, 127.11871337890625, 125.08775329589844, 143.29580688476562, 106.93093872070312, 144.90467834472656, 125.89631652832031, 150.80377197265625, 123.31524658203125, 174.91522216796875, 205.73936462402344, 164.6888427734375, 339.4767761230469, 150.9941864013672, 188.0550537109375, 167.74098205566406, 181.11749267578125, 174.76626586914062, 238.78607177734375, 315.6474914550781, 142.1644287109375, 211.7041473388672, 159.8805389404297, 420.63427734375, 117.5331039428711, 202.92715454101562, 136.34300231933594, 194.4773712158203, 118.80149841308594, 201.4883575439453, 127.2415542602539, 352.0369873046875, 157.41717529296875, 340.117431640625, 1651.041015625, 329.47283935546875, 289.9035949707031, 340.55841064453125, 7386.60009765625, 5080.79833984375, 1793.0244140625]
all layer l1 norms: [178870.296875, 76075.84375, 88914.421875, 65980.7109375, 97689.484375, 400774.4375, 92597.7109375, 51812.93359375, 467879.96875, 271108.5, 175073.0, 50549.08203125, 133352.328125, 49092.875, 190838.96875, 58955.9609375, 156621.40625, 69942.765625, 176059.1875, 74397.1484375, 202055.328125, 83587.6875, 223884.0, 104616.4140625, 226208.21875, 138685.4375, 255102.078125, 117189.78125, 239996.21875, 134356.78125, 274120.5625, 134336.28125, 275887.28125, 230576.890625, 298650.125, 389379.125, 259126.84375, 209699.609375, 284941.53125, 203097.65625, 288103.84375, 289959.84375, 281963.53125, 149135.703125, 240384.84375, 186494.125, 389975.40625, 123905.265625, 236403.203125, 134894.0, 237268.234375, 126883.21875, 252298.875, 137624.0, 264099.84375, 178233.75, 488520.6875, 1653176.5, 393943.0, 325698.0, 416044.34375, 775587.25, 1208335.625, 1646553.875]
all layer l2 norms: [133.5002899169922, 67.95451354980469, 73.90388488769531, 92.54953002929688, 115.25607299804688, 9105.2880859375, 59.86937713623047, 47.773197174072266, 315.0846862792969, 526.5986938476562, 109.98509216308594, 51.996402740478516, 85.21037292480469, 51.53493881225586, 111.59171295166016, 60.48255157470703, 87.95255279541016, 71.53959655761719, 98.12311553955078, 72.80823516845703, 117.51419830322266, 81.17829895019531, 125.77339935302734, 94.42681884765625, 127.11871337890625, 125.08775329589844, 143.29580688476562, 106.93093872070312, 144.90467834472656, 125.89631652832031, 150.80377197265625, 123.31524658203125, 174.91522216796875, 205.73936462402344, 164.6888427734375, 339.4767761230469, 150.9941864013672, 188.0550537109375, 167.74098205566406, 181.11749267578125, 174.76626586914062, 238.78607177734375, 315.6474914550781, 142.1644287109375, 211.7041473388672, 159.8805389404297, 420.63427734375, 117.5331039428711, 202.92715454101562, 136.34300231933594, 194.4773712158203, 118.80149841308594, 201.4883575439453, 127.2415542602539, 352.0369873046875, 157.41717529296875, 340.117431640625, 1651.041015625, 329.47283935546875, 289.9035949707031, 340.55841064453125, 7386.60009765625, 5080.79833984375, 1793.0244140625]
all layer kurtosis: [259.88539179772647, 72.3777720394807, 333.1443875277886, 2471.068812514728, 1353.5010273467162, 67678.1622899009, 57.83384787611856, 237.37633312833375, 110.41460680324872, 4859.211262861251, 44.746094945551604, 14.22337446889011, 445.71587465979104, 11.886790343234793, 14.30666389306215, 13.515235380511967, 9.158547171628273, 13.86142483949353, 5.44277713041013, 12.98818076010589, 52.147175929486075, 8.62666970588379, 24.36308794419846, 39.16324676551839, 20.3101339478206, 175.10412206188496, 26.574058421265036, 18.084747941295028, 168.07320121193584, 88.86437874609094, 3.0566137714965738, 13.312033711268157, 308.0052431604418, 88.6188477381058, 4.376020567624288, 46.72962076502161, 90.54158734899792, 97.00950015516824, 76.75219452203518, 49.54650404067241, 133.92002604514389, 36.9186887351212, 2500.716788882592, 42.65978294528761, 1587.120781990049, 33.871218546702664, 3029.9231873325243, 27.706756408289667, 1398.1303795998979, 370.8426850217793, 1252.6546847017921, 8.597757548167074, 1109.8170423523159, 9.13138980599569, 2991.2675420941, 7.427175903150937, 610.1914823921488, 88.00315861795858, 1390.989116958824, 5094.574119914037, 851.3129694129565, 65984.00562273938, 9001.198309422904, 436.1333106872215]
saving figures to...
outputs/baffo32/decapoda-research-llama-7B-hf/piqa_bs_2/layerwise_condense_sparse_exhausive_sr0.625_trial_1_min_fro/figs/activation
11/15/2023 21:17:09 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1266.14it/s]
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
11/15/2023 21:17:09 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1229.16it/s]
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
starting candidate scanning for layer drop.
dropped layer 0's attn
11/15/2023 21:17:10 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:17:10 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1230.24it/s]
100%|██████████| 3/3 [00:00<00:00, 1263.09it/s]
11/15/2023 21:17:10 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1359.29it/s]
11/15/2023 21:17:10 - INFO - datasets.builder - No config specified, defaulting to the single config: piqa/plain_text
11/15/2023 21:17:10 - INFO - datasets.info - Loading Dataset Infos from /home/lanxiang/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:17:10 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
11/15/2023 21:17:10 - INFO - datasets.builder - Overwrite dataset info from restored data version.
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:17:10 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
100%|██████████| 3/3 [00:00<00:00, 1376.69it/s]
11/15/2023 21:17:10 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
11/15/2023 21:17:10 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011


  0%|          | 0/3 [00:00<?, ?it/s][A[A100%|██████████| 3/3 [00:00<00:00, 1451.99it/s]
Running loglikelihood requests
Running loglikelihood requests
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Running loglikelihood requests
/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
  0%|          | 0/100 [00:00<?, ?it/s]/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(


  0%|          | 0/100 [00:00<?, ?it/s][A[A/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
11/15/2023 21:17:10 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1377.14it/s]
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]/home/lanxiang/anaconda3/envs/llm_train/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(


  1%|          | 1/100 [00:00<01:11,  1.39it/s]  1%|          | 1/100 [00:00<01:10,  1.40it/s]  1%|          | 1/100 [00:00<00:57,  1.72it/s]  1%|          | 1/100 [00:00<01:08,  1.44it/s]  1%|          | 1/100 [00:01<02:00,  1.22s/it]  1%|          | 1/100 [00:01<02:53,  1.76s/it][A[A  1%|          | 1/100 [00:00<01:09,  1.43it/s]  1%|          | 1/100 [00:00<01:11,  1.39it/s]  2%|▏         | 2/100 [00:01<01:00,  1.61it/s]

  2%|▏         | 2/100 [00:01<01:00,  1.62it/s][A[A  2%|▏         | 2/100 [00:01<01:01,  1.59it/s]  2%|▏         | 2/100 [00:01<00:56,  1.75it/s]  2%|▏         | 2/100 [00:02<01:43,  1.06s/it]  2%|▏         | 2/100 [00:01<01:01,  1.59it/s]  2%|▏         | 2/100 [00:01<01:01,  1.59it/s]  2%|▏         | 2/100 [00:01<01:22,  1.20it/s]  3%|▎         | 3/100 [00:01<00:57,  1.69it/s]

  3%|▎         | 3/100 [00:01<00:57,  1.69it/s][A[A  3%|▎         | 3/100 [00:01<00:54,  1.76it/s]  3%|▎         | 3/100 [00:02<01:20,  1.21it/s]  3%|▎         | 3/100 [00:01<00:57,  1.67it/s]  3%|▎         | 3/100 [00:01<00:57,  1.67it/s]  3%|▎         | 3/100 [00:01<00:57,  1.68it/s]  3%|▎         | 3/100 [00:02<01:08,  1.41it/s]  4%|▍         | 4/100 [00:02<00:56,  1.71it/s]

  4%|▍         | 4/100 [00:02<00:56,  1.71it/s][A[A  4%|▍         | 4/100 [00:03<01:09,  1.37it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<01:03,  1.52it/s]  4%|▍         | 4/100 [00:02<00:54,  1.75it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]

  5%|▌         | 5/100 [00:02<00:55,  1.73it/s][A[A  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  5%|▌         | 5/100 [00:04<01:03,  1.49it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  5%|▌         | 5/100 [00:02<00:54,  1.76it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  5%|▌         | 5/100 [00:03<00:59,  1.60it/s]  6%|▌         | 6/100 [00:03<00:53,  1.77it/s]  6%|▌         | 6/100 [00:03<00:53,  1.75it/s]  6%|▌         | 6/100 [00:04<00:59,  1.58it/s]  6%|▌         | 6/100 [00:04<00:56,  1.66it/s]

  6%|▌         | 6/100 [00:03<00:53,  1.74it/s]  6%|▌         | 6/100 [00:03<00:53,  1.75it/s][A[A  6%|▌         | 6/100 [00:03<00:53,  1.74it/s]  6%|▌         | 6/100 [00:03<00:53,  1.74it/s]  7%|▋         | 7/100 [00:05<00:57,  1.62it/s]  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]  7%|▋         | 7/100 [00:03<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.73it/s]  7%|▋         | 7/100 [00:04<00:55,  1.68it/s]  7%|▋         | 7/100 [00:04<00:53,  1.73it/s]  7%|▋         | 7/100 [00:04<00:53,  1.73it/s]

  7%|▋         | 7/100 [00:04<00:53,  1.74it/s][A[A  8%|▊         | 8/100 [00:05<00:55,  1.67it/s]  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]

  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]  8%|▊         | 8/100 [00:04<00:52,  1.75it/s][A[A  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]  8%|▊         | 8/100 [00:05<00:53,  1.71it/s]  9%|▉         | 9/100 [00:06<00:53,  1.69it/s]  9%|▉         | 9/100 [00:05<00:52,  1.75it/s]  9%|▉         | 9/100 [00:05<00:52,  1.74it/s]  9%|▉         | 9/100 [00:05<00:52,  1.75it/s]

  9%|▉         | 9/100 [00:05<00:52,  1.75it/s][A[A  9%|▉         | 9/100 [00:05<00:51,  1.75it/s]  9%|▉         | 9/100 [00:05<00:52,  1.74it/s]  9%|▉         | 9/100 [00:05<00:52,  1.72it/s] 10%|█         | 10/100 [00:06<00:52,  1.71it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s]

 10%|█         | 10/100 [00:05<00:51,  1.74it/s][A[A 10%|█         | 10/100 [00:06<00:52,  1.73it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 11%|█         | 11/100 [00:07<00:51,  1.73it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s]

 11%|█         | 11/100 [00:06<00:50,  1.76it/s][A[A 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:07<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.77it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.77it/s] 12%|█▏        | 12/100 [00:07<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.77it/s]

 12%|█▏        | 12/100 [00:06<00:49,  1.77it/s][A[A 12%|█▏        | 12/100 [00:06<00:49,  1.77it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:08<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s] 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s] 13%|█▎        | 13/100 [00:08<00:49,  1.77it/s]

 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s] 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s][A[A 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s] 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s] 14%|█▍        | 14/100 [00:09<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.78it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.78it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.78it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.78it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.78it/s]

 14%|█▍        | 14/100 [00:08<00:48,  1.78it/s][A[A 14%|█▍        | 14/100 [00:08<00:48,  1.78it/s] 15%|█▌        | 15/100 [00:09<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s] 15%|█▌        | 15/100 [00:09<00:47,  1.78it/s]

 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s][A[A 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s] 16%|█▌        | 16/100 [00:10<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s]

 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s][A[A 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 17%|█▋        | 17/100 [00:10<00:47,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:10<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s]

 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s][A[A 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:11<00:46,  1.77it/s]

 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s][A[A 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.77it/s] 19%|█▉        | 19/100 [00:11<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s]

 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s][A[A 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:11<00:45,  1.78it/s] 20%|██        | 20/100 [00:12<00:44,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s]

 20%|██        | 20/100 [00:11<00:44,  1.79it/s][A[A 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 21%|██        | 21/100 [00:13<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s]

 21%|██        | 21/100 [00:12<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s][A[A 21%|██        | 21/100 [00:11<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s] 22%|██▏       | 22/100 [00:13<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s]

 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s][A[A 22%|██▏       | 22/100 [00:13<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 23%|██▎       | 23/100 [00:14<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:12<00:42,  1.79it/s]

 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s][A[A 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:14<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:14<00:42,  1.79it/s]

 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s][A[A 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 25%|██▌       | 25/100 [00:15<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s]

 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s][A[A 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:15<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:15<00:41,  1.80it/s]

 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s][A[A 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 27%|██▋       | 27/100 [00:16<00:40,  1.81it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.81it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.81it/s]

 27%|██▋       | 27/100 [00:15<00:40,  1.81it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.81it/s][A[A 27%|██▋       | 27/100 [00:15<00:40,  1.81it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.81it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.81it/s] 28%|██▊       | 28/100 [00:16<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:16<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s]

 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s][A[A 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 29%|██▉       | 29/100 [00:17<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s]

 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s][A[A 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 30%|███       | 30/100 [00:18<00:38,  1.80it/s] 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 30%|███       | 30/100 [00:17<00:38,  1.80it/s] 30%|███       | 30/100 [00:16<00:38,  1.80it/s]

 30%|███       | 30/100 [00:16<00:38,  1.80it/s][A[A 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 31%|███       | 31/100 [00:18<00:38,  1.81it/s] 31%|███       | 31/100 [00:18<00:38,  1.81it/s]

 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s][A[A 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 32%|███▏      | 32/100 [00:17<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:19<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s]

 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s][A[A 33%|███▎      | 33/100 [00:19<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s]

 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s][A[A 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:19<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 34%|███▍      | 34/100 [00:20<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s]

 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s][A[A 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 35%|███▌      | 35/100 [00:20<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:20<00:36,  1.78it/s]

 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s][A[A 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 36%|███▌      | 36/100 [00:21<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s]

 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s][A[A 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s]

 37%|███▋      | 37/100 [00:21<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s][A[A 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:21<00:35,  1.79it/s] 38%|███▊      | 38/100 [00:22<00:34,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s]

 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s][A[A 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 39%|███▉      | 39/100 [00:23<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s]

 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s][A[A 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:23<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:23<00:33,  1.80it/s]

 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s][A[A 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 41%|████      | 41/100 [00:24<00:32,  1.81it/s] 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 41%|████      | 41/100 [00:22<00:32,  1.81it/s] 41%|████      | 41/100 [00:23<00:32,  1.81it/s]

 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 41%|████      | 41/100 [00:23<00:32,  1.81it/s][A[A 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:24<00:31,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:31,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:31,  1.81it/s]

 42%|████▏     | 42/100 [00:23<00:31,  1.81it/s][A[A 42%|████▏     | 42/100 [00:24<00:31,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:31,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:31,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:31,  1.81it/s] 43%|████▎     | 43/100 [00:25<00:31,  1.82it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s]

 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s][A[A 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 44%|████▍     | 44/100 [00:25<00:30,  1.82it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 44%|████▍     | 44/100 [00:25<00:30,  1.82it/s]

 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s][A[A 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:26<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s]

 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s][A[A 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:26<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:26<00:29,  1.82it/s]

 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s][A[A 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:27<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s]

 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s][A[A 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.82it/s]

 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s][A[A 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:28<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:28<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s]

 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s][A[A 50%|█████     | 50/100 [00:29<00:27,  1.82it/s] 50%|█████     | 50/100 [00:27<00:27,  1.82it/s] 50%|█████     | 50/100 [00:28<00:27,  1.82it/s]

 50%|█████     | 50/100 [00:28<00:27,  1.82it/s] 50%|█████     | 50/100 [00:28<00:27,  1.82it/s][A[A 50%|█████     | 50/100 [00:28<00:27,  1.82it/s] 50%|█████     | 50/100 [00:28<00:27,  1.82it/s] 50%|█████     | 50/100 [00:28<00:27,  1.82it/s] 51%|█████     | 51/100 [00:29<00:26,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s]

 51%|█████     | 51/100 [00:29<00:26,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s][A[A 51%|█████     | 51/100 [00:28<00:26,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s] 52%|█████▏    | 52/100 [00:30<00:26,  1.83it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.83it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.83it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.83it/s]

 52%|█████▏    | 52/100 [00:29<00:26,  1.83it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.83it/s][A[A 52%|█████▏    | 52/100 [00:29<00:26,  1.83it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.83it/s] 53%|█████▎    | 53/100 [00:30<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s]

 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s][A[A 53%|█████▎    | 53/100 [00:30<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:31<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s]

 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s][A[A 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 55%|█████▌    | 55/100 [00:31<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:31<00:24,  1.82it/s]

 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s][A[A

 56%|█████▌    | 56/100 [00:31<00:24,  1.79it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.79it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.79it/s] 56%|█████▌    | 56/100 [00:32<00:24,  1.79it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.79it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.79it/s][A[A 56%|█████▌    | 56/100 [00:31<00:24,  1.79it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.79it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.80it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.80it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.80it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.80it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.80it/s]

 57%|█████▋    | 57/100 [00:31<00:23,  1.80it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.80it/s][A[A 57%|█████▋    | 57/100 [00:31<00:23,  1.80it/s] 58%|█████▊    | 58/100 [00:33<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s]

 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s][A[A 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 59%|█████▉    | 59/100 [00:34<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:32<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:32<00:22,  1.82it/s]

 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:32<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s][A[A 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:34<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:34<00:22,  1.82it/s]

 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:22,  1.82it/s][A[A 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 61%|██████    | 61/100 [00:35<00:21,  1.82it/s] 61%|██████    | 61/100 [00:33<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s]

 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s][A[A 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 62%|██████▏   | 62/100 [00:35<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s]

 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s][A[A 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:36<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s]

 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s][A[A 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 64%|██████▍   | 64/100 [00:36<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:36<00:19,  1.82it/s]

 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s][A[A 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 65%|██████▌   | 65/100 [00:37<00:19,  1.80it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.80it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.80it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.80it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.80it/s]

 65%|██████▌   | 65/100 [00:36<00:19,  1.80it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.80it/s][A[A 65%|██████▌   | 65/100 [00:36<00:19,  1.80it/s] 66%|██████▌   | 66/100 [00:37<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s]

 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:37<00:18,  1.81it/s][A[A 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:38<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s]

 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s][A[A 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 68%|██████▊   | 68/100 [00:39<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s]

 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s][A[A 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:39<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s]

 69%|██████▉   | 69/100 [00:39<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s][A[A 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 70%|███████   | 70/100 [00:40<00:16,  1.81it/s] 70%|███████   | 70/100 [00:38<00:16,  1.81it/s]

 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s][A[A 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:40<00:15,  1.82it/s] 71%|███████   | 71/100 [00:39<00:15,  1.82it/s]

 71%|███████   | 71/100 [00:39<00:15,  1.82it/s][A[A 71%|███████   | 71/100 [00:39<00:15,  1.82it/s] 71%|███████   | 71/100 [00:40<00:15,  1.82it/s] 71%|███████   | 71/100 [00:39<00:15,  1.82it/s] 71%|███████   | 71/100 [00:39<00:15,  1.82it/s] 71%|███████   | 71/100 [00:39<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:41<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s]

 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s][A[A 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 73%|███████▎  | 73/100 [00:41<00:14,  1.83it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.83it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.83it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.83it/s]

 73%|███████▎  | 73/100 [00:40<00:14,  1.83it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.83it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.83it/s][A[A 73%|███████▎  | 73/100 [00:41<00:14,  1.83it/s] 74%|███████▍  | 74/100 [00:42<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s]

 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s][A[A 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 75%|███████▌  | 75/100 [00:42<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s]

 75%|███████▌  | 75/100 [00:42<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s][A[A 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:43<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s]

 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s][A[A 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s]

 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s][A[A 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s]

 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:44<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s][A[A 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 79%|███████▉  | 79/100 [00:45<00:11,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s]

 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s][A[A 79%|███████▉  | 79/100 [00:43<00:11,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:45<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s]

 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:45<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s][A[A 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s]

 81%|████████  | 81/100 [00:46<00:10,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s][A[A 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 82%|████████▏ | 82/100 [00:46<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:46<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s]

 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s][A[A 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:47<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s]

 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s][A[A 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 84%|████████▍ | 84/100 [00:47<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.83it/s] 84%|████████▍ | 84/100 [00:47<00:08,  1.82it/s]

 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s][A[A 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:48<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s]

 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s][A[A 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s] 86%|████████▌ | 86/100 [00:48<00:07,  1.81it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s]

 86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s][A[A 86%|████████▌ | 86/100 [00:48<00:07,  1.81it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s] 87%|████████▋ | 87/100 [00:49<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s]

 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s][A[A 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 88%|████████▊ | 88/100 [00:50<00:06,  1.82it/s]

 88%|████████▊ | 88/100 [00:48<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.82it/s][A[A 88%|████████▊ | 88/100 [00:48<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:50<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s]

 89%|████████▉ | 89/100 [00:50<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s][A[A 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 90%|█████████ | 90/100 [00:51<00:05,  1.81it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 90%|█████████ | 90/100 [00:49<00:05,  1.81it/s]

 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s][A[A 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 91%|█████████ | 91/100 [00:51<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s]

 91%|█████████ | 91/100 [00:51<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s][A[A 92%|█████████▏| 92/100 [00:52<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s]

 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s][A[A 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 93%|█████████▎| 93/100 [00:52<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s]

 93%|█████████▎| 93/100 [00:52<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s][A[A 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:53<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s]

 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s][A[A 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 95%|█████████▌| 95/100 [00:53<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s]

 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s][A[A 95%|█████████▌| 95/100 [00:53<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 96%|█████████▌| 96/100 [00:54<00:02,  1.82it/s]

 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s][A[A 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s]

 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s][A[A 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:55<00:01,  1.83it/s]

 98%|█████████▊| 98/100 [00:54<00:01,  1.83it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.83it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.83it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.83it/s][A[A 98%|█████████▊| 98/100 [00:54<00:01,  1.83it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.83it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.83it/s] 99%|█████████▉| 99/100 [00:56<00:00,  1.82it/s]

 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s][A[A 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s] 99%|█████████▉| 99/100 [00:54<00:00,  1.82it/s]100%|██████████| 100/100 [00:56<00:00,  1.83it/s]

100%|██████████| 100/100 [00:56<00:00,  1.77it/s]
100%|██████████| 100/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.83it/s][A[A100%|██████████| 100/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:56<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.83it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:56<00:00,  1.78it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]


100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
eval result: 0.5
dropped layer 0's ffn
11/15/2023 21:18:06 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1102.12it/s]
11/15/2023 21:18:06 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1259.05it/s]
11/15/2023 21:18:06 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1177.40it/s]
11/15/2023 21:18:06 - INFO - datasets.builder - No config specified, defaulting to the single config: piqa/plain_text
11/15/2023 21:18:06 - INFO - datasets.info - Loading Dataset Infos from /home/lanxiang/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:18:06 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
Running loglikelihood requests
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1114.62it/s]
Running loglikelihood requests
11/15/2023 21:18:06 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/15/2023 21:18:06 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:18:06 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
11/15/2023 21:18:06 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011


  0%|          | 0/3 [00:00<?, ?it/s][A[A100%|██████████| 3/3 [00:00<00:00, 1241.53it/s]
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
11/15/2023 21:18:06 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1240.43it/s]
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]11/15/2023 21:18:06 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 563.40it/s]


  0%|          | 0/100 [00:00<?, ?it/s][A[ARunning loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]11/15/2023 21:18:06 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1063.02it/s]
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:05,  1.51it/s]  1%|          | 1/100 [00:00<01:07,  1.47it/s]

  1%|          | 1/100 [00:00<01:10,  1.39it/s]  1%|          | 1/100 [00:00<01:15,  1.31it/s][A[A  1%|          | 1/100 [00:00<01:12,  1.37it/s]  1%|          | 1/100 [00:00<01:16,  1.30it/s]  1%|          | 1/100 [00:00<00:57,  1.72it/s]  1%|          | 1/100 [00:00<01:14,  1.33it/s]  2%|▏         | 2/100 [00:01<00:59,  1.64it/s]

  2%|▏         | 2/100 [00:01<01:03,  1.55it/s]  2%|▏         | 2/100 [00:01<01:03,  1.55it/s]  2%|▏         | 2/100 [00:01<01:01,  1.60it/s][A[A  2%|▏         | 2/100 [00:01<01:01,  1.59it/s]  2%|▏         | 2/100 [00:01<00:55,  1.76it/s]  2%|▏         | 2/100 [00:01<01:02,  1.56it/s]  2%|▏         | 2/100 [00:01<00:59,  1.66it/s]  3%|▎         | 3/100 [00:01<00:58,  1.66it/s]  3%|▎         | 3/100 [00:01<00:56,  1.71it/s]  3%|▎         | 3/100 [00:01<00:57,  1.69it/s]

  3%|▎         | 3/100 [00:01<00:58,  1.67it/s][A[A  3%|▎         | 3/100 [00:01<00:59,  1.64it/s]  3%|▎         | 3/100 [00:01<00:59,  1.64it/s]  3%|▎         | 3/100 [00:01<00:55,  1.76it/s]  3%|▎         | 3/100 [00:01<00:58,  1.65it/s]  4%|▍         | 4/100 [00:02<00:55,  1.73it/s]  4%|▍         | 4/100 [00:02<00:56,  1.69it/s]  4%|▍         | 4/100 [00:02<00:55,  1.72it/s]

  4%|▍         | 4/100 [00:02<00:56,  1.69it/s]  4%|▍         | 4/100 [00:02<00:56,  1.71it/s][A[A  4%|▍         | 4/100 [00:02<00:56,  1.69it/s]  4%|▍         | 4/100 [00:02<00:54,  1.76it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  5%|▌         | 5/100 [00:02<00:54,  1.74it/s]  5%|▌         | 5/100 [00:02<00:54,  1.74it/s]  5%|▌         | 5/100 [00:03<00:55,  1.71it/s]

  5%|▌         | 5/100 [00:02<00:55,  1.73it/s][A[A  5%|▌         | 5/100 [00:03<00:55,  1.72it/s]  5%|▌         | 5/100 [00:03<00:55,  1.72it/s]  5%|▌         | 5/100 [00:02<00:53,  1.76it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  6%|▌         | 6/100 [00:03<00:53,  1.74it/s]  6%|▌         | 6/100 [00:03<00:54,  1.73it/s]  6%|▌         | 6/100 [00:03<00:54,  1.74it/s]  6%|▌         | 6/100 [00:03<00:54,  1.73it/s]

  6%|▌         | 6/100 [00:03<00:54,  1.73it/s][A[A  6%|▌         | 6/100 [00:03<00:54,  1.73it/s]  6%|▌         | 6/100 [00:03<00:53,  1.76it/s]  6%|▌         | 6/100 [00:03<00:54,  1.73it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]

  7%|▋         | 7/100 [00:04<00:53,  1.74it/s][A[A  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]  7%|▋         | 7/100 [00:03<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]  8%|▊         | 8/100 [00:04<00:51,  1.77it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]

  8%|▊         | 8/100 [00:04<00:52,  1.76it/s][A[A  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.75it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.75it/s]  9%|▉         | 9/100 [00:05<00:51,  1.75it/s]

  9%|▉         | 9/100 [00:05<00:51,  1.76it/s][A[A  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.76it/s] 10%|█         | 10/100 [00:05<00:51,  1.76it/s] 10%|█         | 10/100 [00:05<00:51,  1.76it/s] 10%|█         | 10/100 [00:05<00:51,  1.76it/s] 10%|█         | 10/100 [00:05<00:51,  1.76it/s]

 10%|█         | 10/100 [00:05<00:51,  1.76it/s][A[A 10%|█         | 10/100 [00:05<00:51,  1.76it/s] 10%|█         | 10/100 [00:05<00:51,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s]

 11%|█         | 11/100 [00:06<00:50,  1.77it/s][A[A 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s]

 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s][A[A 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 13%|█▎        | 13/100 [00:07<00:48,  1.79it/s] 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s] 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s] 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s] 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s]

 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s][A[A 13%|█▎        | 13/100 [00:07<00:48,  1.78it/s] 13%|█▎        | 13/100 [00:07<00:48,  1.79it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.79it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.79it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.79it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.79it/s]

 14%|█▍        | 14/100 [00:08<00:48,  1.79it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.79it/s][A[A 14%|█▍        | 14/100 [00:07<00:48,  1.79it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.79it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s]

 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s][A[A 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s]

 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s][A[A 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s]

 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s][A[A 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s]

 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s][A[A 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s]

 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s][A[A 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s]

 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s][A[A 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:12<00:44,  1.78it/s] 21%|██        | 21/100 [00:12<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:12<00:44,  1.78it/s]

 21%|██        | 21/100 [00:11<00:44,  1.78it/s][A[A 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s]

 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s][A[A 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s]

 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s][A[A 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:12<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s]

 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s][A[A 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s]

 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s][A[A 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s]

 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s][A[A 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s]

 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s][A[A 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s]

 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s][A[A 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.78it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.78it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.78it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.78it/s]

 29%|██▉       | 29/100 [00:16<00:39,  1.78it/s][A[A 29%|██▉       | 29/100 [00:16<00:39,  1.78it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.78it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.78it/s] 30%|███       | 30/100 [00:16<00:39,  1.78it/s] 30%|███       | 30/100 [00:17<00:39,  1.78it/s] 30%|███       | 30/100 [00:17<00:39,  1.78it/s] 30%|███       | 30/100 [00:17<00:39,  1.78it/s]

 30%|███       | 30/100 [00:16<00:39,  1.78it/s] 30%|███       | 30/100 [00:17<00:39,  1.78it/s][A[A 30%|███       | 30/100 [00:16<00:39,  1.78it/s] 30%|███       | 30/100 [00:17<00:39,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s]

 31%|███       | 31/100 [00:17<00:38,  1.78it/s][A[A 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s]

 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s][A[A 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 32%|███▏      | 32/100 [00:17<00:37,  1.79it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s]

 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s][A[A 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.79it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.79it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.79it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.79it/s]

 34%|███▍      | 34/100 [00:19<00:36,  1.79it/s][A[A 34%|███▍      | 34/100 [00:19<00:36,  1.79it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.79it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.80it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.80it/s]

 35%|███▌      | 35/100 [00:19<00:36,  1.80it/s][A[A 35%|███▌      | 35/100 [00:19<00:36,  1.80it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.80it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.80it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.80it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s]

 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s][A[A 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s]

 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s][A[A 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s]

 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s][A[A 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.79it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s]

 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s][A[A 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s]

 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s][A[A 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s]

 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s][A[A 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s]

 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s][A[A 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s]

 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s][A[A 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s]

 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s][A[A 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s]

 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s][A[A 46%|████▌     | 46/100 [00:25<00:29,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.80it/s]

 46%|████▌     | 46/100 [00:25<00:29,  1.80it/s][A[A 46%|████▌     | 46/100 [00:25<00:29,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.79it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.79it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.79it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.79it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.79it/s]

 47%|████▋     | 47/100 [00:26<00:29,  1.79it/s][A[A 47%|████▋     | 47/100 [00:26<00:29,  1.79it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.79it/s] 48%|████▊     | 48/100 [00:27<00:29,  1.78it/s] 48%|████▊     | 48/100 [00:27<00:29,  1.78it/s] 48%|████▊     | 48/100 [00:27<00:29,  1.78it/s]

 48%|████▊     | 48/100 [00:27<00:29,  1.78it/s] 48%|████▊     | 48/100 [00:27<00:29,  1.78it/s][A[A 48%|████▊     | 48/100 [00:27<00:29,  1.78it/s] 48%|████▊     | 48/100 [00:26<00:29,  1.78it/s] 48%|████▊     | 48/100 [00:27<00:29,  1.78it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s]

 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s][A[A 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s]

 50%|█████     | 50/100 [00:28<00:27,  1.80it/s][A[A 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s]

 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s][A[A 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s]

 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s][A[A 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s]

 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s][A[A 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s]

 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s][A[A 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s]

 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s][A[A 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.83it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.83it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.83it/s]

 56%|█████▌    | 56/100 [00:31<00:24,  1.83it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.83it/s][A[A 56%|█████▌    | 56/100 [00:31<00:24,  1.83it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.83it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.83it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s]

 57%|█████▋    | 57/100 [00:32<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s][A[A 57%|█████▋    | 57/100 [00:32<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s]

 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s][A[A 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s]

 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s][A[A 59%|█████▉    | 59/100 [00:32<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s] 60%|██████    | 60/100 [00:33<00:21,  1.83it/s] 60%|██████    | 60/100 [00:33<00:21,  1.83it/s] 60%|██████    | 60/100 [00:33<00:21,  1.83it/s]

 60%|██████    | 60/100 [00:33<00:21,  1.83it/s] 60%|██████    | 60/100 [00:33<00:21,  1.83it/s] 60%|██████    | 60/100 [00:33<00:21,  1.83it/s][A[A 60%|██████    | 60/100 [00:33<00:21,  1.83it/s] 60%|██████    | 60/100 [00:33<00:21,  1.83it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s]

 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s][A[A 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s]

 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s][A[A 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s]

 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s][A[A 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:20,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:20,  1.80it/s]

 64%|██████▍   | 64/100 [00:35<00:20,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:20,  1.80it/s][A[A 64%|██████▍   | 64/100 [00:35<00:20,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:20,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:20,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:20,  1.80it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s]

 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s][A[A 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s]

 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s][A[A 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s]

 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s][A[A 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.80it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.80it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.80it/s]

 68%|██████▊   | 68/100 [00:38<00:17,  1.80it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.80it/s][A[A 68%|██████▊   | 68/100 [00:38<00:17,  1.80it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.80it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.80it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s]

 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s][A[A 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s]

 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s][A[A 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:15,  1.82it/s] 71%|███████   | 71/100 [00:39<00:15,  1.82it/s] 71%|███████   | 71/100 [00:39<00:15,  1.82it/s]

 71%|███████   | 71/100 [00:39<00:15,  1.82it/s] 71%|███████   | 71/100 [00:39<00:15,  1.82it/s][A[A 71%|███████   | 71/100 [00:39<00:15,  1.82it/s] 71%|███████   | 71/100 [00:39<00:15,  1.82it/s] 71%|███████   | 71/100 [00:39<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s]

 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s][A[A 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s]

 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s][A[A 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s]

 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s][A[A 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s]

 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s][A[A 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s]

 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s][A[A 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s]

 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s][A[A 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.83it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.83it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.83it/s]

 78%|███████▊  | 78/100 [00:43<00:12,  1.83it/s][A[A 78%|███████▊  | 78/100 [00:43<00:12,  1.83it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.83it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.83it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.83it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s]

 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s][A[A 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s] 79%|███████▉  | 79/100 [00:43<00:11,  1.83it/s] 80%|████████  | 80/100 [00:44<00:11,  1.82it/s]

 80%|████████  | 80/100 [00:44<00:11,  1.82it/s][A[A 80%|████████  | 80/100 [00:44<00:11,  1.82it/s] 80%|████████  | 80/100 [00:44<00:11,  1.82it/s] 80%|████████  | 80/100 [00:44<00:11,  1.82it/s] 80%|████████  | 80/100 [00:44<00:11,  1.82it/s] 80%|████████  | 80/100 [00:44<00:11,  1.82it/s] 80%|████████  | 80/100 [00:44<00:11,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s]

 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s][A[A 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s]

 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s][A[A 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s]

 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s][A[A 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s]

 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s][A[A 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s]

 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s][A[A 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s]

 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s][A[A 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s]

 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s][A[A 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s]

 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s][A[A 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s]

 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s][A[A 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s]

 90%|█████████ | 90/100 [00:49<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s][A[A 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s]

 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s][A[A 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s]

 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s][A[A 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s]

 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s][A[A 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s]

 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s][A[A 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s]

 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s][A[A 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.81it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.81it/s]

 96%|█████████▌| 96/100 [00:53<00:02,  1.81it/s][A[A 96%|█████████▌| 96/100 [00:53<00:02,  1.81it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.81it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.81it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.81it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.81it/s]

 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s][A[A 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s]

 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s][A[A 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s]

 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s][A[A 99%|█████████▉| 99/100 [00:54<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.81it/s]

100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.81it/s][A[A100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.81it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
eval result: 0.5
dropped layer 1's attn
11/15/2023 21:19:02 - INFO - datasets.builder - No config specified, defaulting to the single config: piqa/plain_text
11/15/2023 21:19:02 - INFO - datasets.info - Loading Dataset Infos from /home/lanxiang/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:19:02 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1198.60it/s]11/15/2023 21:19:02 - INFO - datasets.builder - Overwrite dataset info from restored data version.

11/15/2023 21:19:02 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:19:02 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
11/15/2023 21:19:02 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011


  0%|          | 0/3 [00:00<?, ?it/s][A[A11/15/2023 21:19:02 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1206.42it/s]
100%|██████████| 3/3 [00:00<00:00, 991.80it/s]
11/15/2023 21:19:02 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:19:02 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1157.58it/s]
100%|██████████| 3/3 [00:00<00:00, 1089.71it/s]
11/15/2023 21:19:02 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1189.87it/s]
11/15/2023 21:19:02 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]Running loglikelihood requests
100%|██████████| 3/3 [00:00<00:00, 1256.16it/s]
Running loglikelihood requests
Running loglikelihood requests
Running loglikelihood requests
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]

  0%|          | 0/100 [00:00<?, ?it/s][A[A11/15/2023 21:19:02 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]Running loglikelihood requests
100%|██████████| 3/3 [00:00<00:00, 1269.72it/s]
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:01,  1.60it/s]  1%|          | 1/100 [00:00<00:59,  1.67it/s]

  1%|          | 1/100 [00:00<01:05,  1.52it/s][A[A  1%|          | 1/100 [00:00<01:05,  1.51it/s]  1%|          | 1/100 [00:00<01:02,  1.57it/s]  1%|          | 1/100 [00:00<01:04,  1.54it/s]  1%|          | 1/100 [00:00<01:04,  1.54it/s]  1%|          | 1/100 [00:00<01:04,  1.53it/s]  2%|▏         | 2/100 [00:01<00:58,  1.69it/s]  2%|▏         | 2/100 [00:01<00:56,  1.72it/s]  2%|▏         | 2/100 [00:01<00:59,  1.65it/s]

  2%|▏         | 2/100 [00:01<00:59,  1.65it/s][A[A  2%|▏         | 2/100 [00:01<00:58,  1.68it/s]  2%|▏         | 2/100 [00:01<00:58,  1.67it/s]  2%|▏         | 2/100 [00:01<00:59,  1.66it/s]  2%|▏         | 2/100 [00:01<00:59,  1.66it/s]  3%|▎         | 3/100 [00:01<00:57,  1.70it/s]  3%|▎         | 3/100 [00:01<00:56,  1.73it/s]

  3%|▎         | 3/100 [00:01<00:57,  1.69it/s][A[A  3%|▎         | 3/100 [00:01<00:56,  1.71it/s]  3%|▎         | 3/100 [00:01<00:57,  1.70it/s]  3%|▎         | 3/100 [00:01<00:57,  1.69it/s]  3%|▎         | 3/100 [00:01<00:56,  1.71it/s]  3%|▎         | 3/100 [00:01<00:57,  1.70it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:56,  1.69it/s]  4%|▍         | 4/100 [00:02<00:55,  1.72it/s]  4%|▍         | 4/100 [00:02<00:56,  1.71it/s]

  4%|▍         | 4/100 [00:02<00:56,  1.69it/s][A[A  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  5%|▌         | 5/100 [00:02<00:54,  1.73it/s]  5%|▌         | 5/100 [00:02<00:54,  1.74it/s]

  5%|▌         | 5/100 [00:02<00:54,  1.73it/s][A[A  5%|▌         | 5/100 [00:02<00:55,  1.73it/s]  5%|▌         | 5/100 [00:02<00:54,  1.74it/s]  5%|▌         | 5/100 [00:02<00:54,  1.73it/s]  5%|▌         | 5/100 [00:02<00:54,  1.73it/s]  5%|▌         | 5/100 [00:02<00:54,  1.73it/s]  6%|▌         | 6/100 [00:03<00:54,  1.74it/s]  6%|▌         | 6/100 [00:03<00:53,  1.74it/s]  6%|▌         | 6/100 [00:03<00:54,  1.74it/s]  6%|▌         | 6/100 [00:03<00:54,  1.74it/s]

  6%|▌         | 6/100 [00:03<00:54,  1.74it/s][A[A  6%|▌         | 6/100 [00:03<00:53,  1.75it/s]  6%|▌         | 6/100 [00:03<00:54,  1.74it/s]  6%|▌         | 6/100 [00:03<00:54,  1.73it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]

  7%|▋         | 7/100 [00:04<00:53,  1.75it/s][A[A  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]

  8%|▊         | 8/100 [00:04<00:52,  1.76it/s][A[A  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.77it/s]  9%|▉         | 9/100 [00:05<00:51,  1.77it/s]  9%|▉         | 9/100 [00:05<00:51,  1.77it/s]  9%|▉         | 9/100 [00:05<00:51,  1.77it/s]  9%|▉         | 9/100 [00:05<00:51,  1.77it/s]

  9%|▉         | 9/100 [00:05<00:51,  1.77it/s]  9%|▉         | 9/100 [00:05<00:51,  1.77it/s][A[A  9%|▉         | 9/100 [00:05<00:51,  1.77it/s] 10%|█         | 10/100 [00:05<00:50,  1.78it/s] 10%|█         | 10/100 [00:05<00:50,  1.78it/s] 10%|█         | 10/100 [00:05<00:50,  1.78it/s] 10%|█         | 10/100 [00:05<00:50,  1.77it/s]

 10%|█         | 10/100 [00:05<00:50,  1.77it/s][A[A 10%|█         | 10/100 [00:05<00:50,  1.78it/s] 10%|█         | 10/100 [00:05<00:50,  1.78it/s] 10%|█         | 10/100 [00:05<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s]

 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s][A[A 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.77it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s]

 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s][A[A 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.78it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s]

 13%|█▎        | 13/100 [00:07<00:49,  1.78it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s][A[A 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.78it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.78it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.78it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.78it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.78it/s]

 14%|█▍        | 14/100 [00:08<00:48,  1.78it/s][A[A 14%|█▍        | 14/100 [00:08<00:48,  1.78it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s]

 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s][A[A 15%|█▌        | 15/100 [00:08<00:47,  1.79it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s]

 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s][A[A 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 16%|█▌        | 16/100 [00:09<00:46,  1.79it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s]

 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s][A[A 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s]

 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s][A[A 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s]

 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s][A[A 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s]

 20%|██        | 20/100 [00:11<00:45,  1.77it/s][A[A 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s]

 21%|██        | 21/100 [00:11<00:44,  1.78it/s][A[A 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s]

 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s][A[A 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:12<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s]

 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s][A[A 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s]

 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s][A[A 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.78it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.78it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.78it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.78it/s]

 25%|██▌       | 25/100 [00:14<00:42,  1.78it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.78it/s][A[A 25%|██▌       | 25/100 [00:14<00:42,  1.78it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.78it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s]

 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s][A[A 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s]

 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s][A[A 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s]

 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s][A[A 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s]

 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s][A[A 30%|███       | 30/100 [00:16<00:38,  1.81it/s] 30%|███       | 30/100 [00:16<00:38,  1.81it/s] 30%|███       | 30/100 [00:16<00:38,  1.81it/s] 30%|███       | 30/100 [00:16<00:38,  1.81it/s]

 30%|███       | 30/100 [00:16<00:38,  1.81it/s][A[A 30%|███       | 30/100 [00:16<00:38,  1.81it/s] 30%|███       | 30/100 [00:16<00:38,  1.81it/s] 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s]

 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s][A[A 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:17<00:37,  1.80it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s]

 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s][A[A 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s]

 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s][A[A 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s]

 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s][A[A 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s]

 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s][A[A 35%|███▌      | 35/100 [00:19<00:36,  1.81it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s]

 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s][A[A 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s]

 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s][A[A 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s]

 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s][A[A 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 39%|███▉      | 39/100 [00:21<00:34,  1.77it/s] 39%|███▉      | 39/100 [00:21<00:34,  1.77it/s] 39%|███▉      | 39/100 [00:21<00:34,  1.77it/s]

 39%|███▉      | 39/100 [00:21<00:34,  1.77it/s][A[A 39%|███▉      | 39/100 [00:21<00:34,  1.78it/s] 39%|███▉      | 39/100 [00:21<00:34,  1.77it/s] 39%|███▉      | 39/100 [00:21<00:34,  1.77it/s] 39%|███▉      | 39/100 [00:21<00:34,  1.77it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s]

 40%|████      | 40/100 [00:22<00:33,  1.79it/s][A[A 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 41%|████      | 41/100 [00:23<00:33,  1.78it/s] 41%|████      | 41/100 [00:23<00:33,  1.79it/s] 41%|████      | 41/100 [00:23<00:33,  1.78it/s] 41%|████      | 41/100 [00:23<00:33,  1.78it/s]

 41%|████      | 41/100 [00:23<00:33,  1.78it/s] 41%|████      | 41/100 [00:23<00:33,  1.78it/s][A[A 41%|████      | 41/100 [00:23<00:33,  1.78it/s] 41%|████      | 41/100 [00:23<00:33,  1.78it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s]

 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s][A[A 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s]

 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s][A[A 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s]

 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s][A[A 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s]

 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s][A[A 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.79it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s]

 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s][A[A 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.79it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s]

 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s][A[A 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.79it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.79it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.79it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.79it/s]

 48%|████▊     | 48/100 [00:26<00:29,  1.79it/s][A[A 48%|████▊     | 48/100 [00:26<00:28,  1.79it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.79it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.79it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.79it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.79it/s]

 49%|████▉     | 49/100 [00:27<00:28,  1.79it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.79it/s][A[A 49%|████▉     | 49/100 [00:27<00:28,  1.79it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.79it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.79it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s]

 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s][A[A 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s]

 51%|█████     | 51/100 [00:28<00:27,  1.81it/s][A[A 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.79it/s]

 52%|█████▏    | 52/100 [00:29<00:26,  1.79it/s][A[A 52%|█████▏    | 52/100 [00:29<00:26,  1.79it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.79it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.79it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.79it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.79it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.79it/s] 53%|█████▎    | 53/100 [00:29<00:26,  1.80it/s] 53%|█████▎    | 53/100 [00:29<00:26,  1.80it/s]

 53%|█████▎    | 53/100 [00:29<00:26,  1.80it/s] 53%|█████▎    | 53/100 [00:29<00:26,  1.80it/s][A[A 53%|█████▎    | 53/100 [00:29<00:26,  1.80it/s] 53%|█████▎    | 53/100 [00:29<00:26,  1.80it/s] 53%|█████▎    | 53/100 [00:29<00:26,  1.80it/s] 53%|█████▎    | 53/100 [00:29<00:26,  1.80it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s]

 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s][A[A 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 55%|█████▌    | 55/100 [00:30<00:25,  1.79it/s] 55%|█████▌    | 55/100 [00:30<00:25,  1.79it/s] 55%|█████▌    | 55/100 [00:30<00:25,  1.79it/s] 55%|█████▌    | 55/100 [00:30<00:25,  1.79it/s] 55%|█████▌    | 55/100 [00:30<00:25,  1.79it/s]

 55%|█████▌    | 55/100 [00:30<00:25,  1.79it/s] 55%|█████▌    | 55/100 [00:30<00:25,  1.79it/s][A[A 55%|█████▌    | 55/100 [00:30<00:25,  1.79it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.80it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.80it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.80it/s]

 56%|█████▌    | 56/100 [00:31<00:24,  1.80it/s][A[A 56%|█████▌    | 56/100 [00:31<00:24,  1.80it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.80it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.80it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.80it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.79it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.80it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.79it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.79it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.80it/s]

 57%|█████▋    | 57/100 [00:31<00:23,  1.80it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.79it/s][A[A 57%|█████▋    | 57/100 [00:31<00:23,  1.79it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s]

 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s][A[A 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s]

 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s][A[A 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s]

 60%|██████    | 60/100 [00:33<00:22,  1.81it/s][A[A 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s]

 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s][A[A 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s]

 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s][A[A 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s]

 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s][A[A 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s]

 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s][A[A 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s]

 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s][A[A 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s]

 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s][A[A 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s]

 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s][A[A 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s]

 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s][A[A 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s]

 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s][A[A 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.81it/s]

 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s][A[A 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 71%|███████   | 71/100 [00:39<00:15,  1.83it/s] 71%|███████   | 71/100 [00:39<00:15,  1.83it/s]

 71%|███████   | 71/100 [00:39<00:15,  1.83it/s][A[A 71%|███████   | 71/100 [00:39<00:15,  1.83it/s] 71%|███████   | 71/100 [00:39<00:15,  1.83it/s] 71%|███████   | 71/100 [00:39<00:15,  1.83it/s] 71%|███████   | 71/100 [00:39<00:15,  1.83it/s] 71%|███████   | 71/100 [00:39<00:15,  1.83it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.83it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.83it/s]

 72%|███████▏  | 72/100 [00:40<00:15,  1.83it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.83it/s][A[A 72%|███████▏  | 72/100 [00:40<00:15,  1.83it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.83it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.83it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.83it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s]

 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s][A[A 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s]

 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s][A[A 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.83it/s]

 75%|███████▌  | 75/100 [00:41<00:13,  1.83it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.83it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.83it/s][A[A 75%|███████▌  | 75/100 [00:41<00:13,  1.83it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.83it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.83it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.83it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s]

 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s][A[A 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s]

 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.80it/s][A[A 77%|███████▋  | 77/100 [00:43<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.79it/s]

 78%|███████▊  | 78/100 [00:43<00:12,  1.79it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.79it/s][A[A 78%|███████▊  | 78/100 [00:43<00:12,  1.79it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.79it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.79it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.79it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.79it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s]

 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s][A[A 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s]

 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s][A[A 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s]

 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s][A[A 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s]

 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s][A[A 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s]

 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s][A[A 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s]

 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s][A[A 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s]

 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s][A[A 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s]

 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s][A[A 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s]

 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s][A[A 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s]

 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s][A[A 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.80it/s]

 89%|████████▉ | 89/100 [00:49<00:06,  1.80it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.80it/s][A[A 89%|████████▉ | 89/100 [00:49<00:06,  1.80it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.80it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.80it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.80it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.80it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s]

 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s][A[A 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s]

 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s][A[A 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s]

 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s][A[A 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s]

 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s][A[A 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s]

 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s][A[A 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s]

 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s][A[A 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s]

 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s][A[A 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.83it/s]

 97%|█████████▋| 97/100 [00:53<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.83it/s][A[A 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s]

 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s][A[A 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s]

 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s][A[A 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s]

100%|██████████| 100/100 [00:55<00:00,  1.82it/s][A[A100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s]
100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]

100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
eval result: 0.6
dropped layer 1's ffn
11/15/2023 21:19:58 - INFO - datasets.builder - No config specified, defaulting to the single config: piqa/plain_text
11/15/2023 21:19:58 - INFO - datasets.info - Loading Dataset Infos from /home/lanxiang/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:19:58 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/15/2023 21:19:58 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:19:58 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
11/15/2023 21:19:58 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011


  0%|          | 0/3 [00:00<?, ?it/s][A[A100%|██████████| 3/3 [00:00<00:00, 1276.29it/s]
11/15/2023 21:19:58 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1160.57it/s]
11/15/2023 21:19:58 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 866.95it/s]
11/15/2023 21:19:58 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1282.40it/s]
Running loglikelihood requests
11/15/2023 21:19:58 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1176.85it/s]
Running loglikelihood requests
11/15/2023 21:19:58 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1253.03it/s]
Running loglikelihood requests


  0%|          | 0/100 [00:00<?, ?it/s][A[A11/15/2023 21:19:58 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:19:58 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 751.35it/s]
100%|██████████| 3/3 [00:00<00:00, 884.44it/s]
  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]

  1%|          | 1/100 [00:00<01:02,  1.58it/s]  1%|          | 1/100 [00:00<01:06,  1.48it/s][A[A  1%|          | 1/100 [00:00<01:01,  1.62it/s]  1%|          | 1/100 [00:00<01:05,  1.52it/s]  1%|          | 1/100 [00:00<01:00,  1.64it/s]  1%|          | 1/100 [00:00<00:58,  1.70it/s]  1%|          | 1/100 [00:00<01:06,  1.49it/s]  1%|          | 1/100 [00:00<00:58,  1.70it/s]

  2%|▏         | 2/100 [00:01<00:59,  1.65it/s]  2%|▏         | 2/100 [00:01<01:01,  1.61it/s][A[A  2%|▏         | 2/100 [00:01<01:00,  1.62it/s]  2%|▏         | 2/100 [00:01<00:58,  1.68it/s]  2%|▏         | 2/100 [00:01<00:58,  1.67it/s]  2%|▏         | 2/100 [00:01<01:00,  1.61it/s]  2%|▏         | 2/100 [00:01<00:57,  1.71it/s]  2%|▏         | 2/100 [00:01<00:57,  1.71it/s]  3%|▎         | 3/100 [00:01<00:56,  1.72it/s]  3%|▎         | 3/100 [00:01<00:56,  1.71it/s]

  3%|▎         | 3/100 [00:01<00:57,  1.68it/s][A[A  3%|▎         | 3/100 [00:01<00:56,  1.72it/s]  3%|▎         | 3/100 [00:01<00:57,  1.69it/s]  3%|▎         | 3/100 [00:01<00:57,  1.68it/s]  3%|▎         | 3/100 [00:01<00:55,  1.74it/s]  3%|▎         | 3/100 [00:01<00:55,  1.74it/s]

  4%|▍         | 4/100 [00:02<00:55,  1.72it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s][A[A  4%|▍         | 4/100 [00:02<00:55,  1.72it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:55,  1.73it/s]  4%|▍         | 4/100 [00:02<00:55,  1.74it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:55,  1.72it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]

  5%|▌         | 5/100 [00:02<00:55,  1.71it/s][A[A  5%|▌         | 5/100 [00:02<00:55,  1.71it/s]  5%|▌         | 5/100 [00:02<00:55,  1.73it/s]  5%|▌         | 5/100 [00:02<00:55,  1.71it/s]  5%|▌         | 5/100 [00:02<00:54,  1.73it/s]  5%|▌         | 5/100 [00:02<00:54,  1.74it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  6%|▌         | 6/100 [00:03<00:53,  1.74it/s]

  6%|▌         | 6/100 [00:03<00:53,  1.74it/s]  6%|▌         | 6/100 [00:03<00:54,  1.73it/s][A[A  6%|▌         | 6/100 [00:03<00:54,  1.73it/s]  6%|▌         | 6/100 [00:03<00:53,  1.75it/s]  6%|▌         | 6/100 [00:03<00:53,  1.75it/s]  6%|▌         | 6/100 [00:03<00:53,  1.74it/s]  6%|▌         | 6/100 [00:03<00:54,  1.73it/s]  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]

  7%|▋         | 7/100 [00:04<00:53,  1.74it/s][A[A  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]

  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.75it/s][A[A  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.75it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]

  9%|▉         | 9/100 [00:05<00:51,  1.75it/s][A[A  9%|▉         | 9/100 [00:05<00:51,  1.75it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s]

 10%|█         | 10/100 [00:05<00:51,  1.75it/s][A[A 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s]

 11%|█         | 11/100 [00:06<00:51,  1.74it/s][A[A 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s]

 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s][A[A 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s]

 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s][A[A 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s]

 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s][A[A 14%|█▍        | 14/100 [00:07<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s]

 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s][A[A 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.78it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s]

 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s][A[A 16%|█▌        | 16/100 [00:09<00:47,  1.76it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.76it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s]

 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s][A[A 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.77it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.77it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.77it/s]

 18%|█▊        | 18/100 [00:10<00:46,  1.77it/s][A[A 18%|█▊        | 18/100 [00:10<00:46,  1.77it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.77it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.77it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s]

 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s][A[A 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s]

 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s][A[A 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:12<00:44,  1.78it/s]

 21%|██        | 21/100 [00:12<00:44,  1.78it/s][A[A 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:12<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s]

 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s][A[A 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s]

 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s][A[A 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s]

 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s][A[A 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s]

 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s][A[A 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s]

 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s][A[A 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.80it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.80it/s]

 27%|██▋       | 27/100 [00:15<00:40,  1.80it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.80it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.80it/s][A[A 27%|██▋       | 27/100 [00:15<00:40,  1.80it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.80it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:39,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:39,  1.80it/s]

 28%|██▊       | 28/100 [00:15<00:39,  1.80it/s][A[A 28%|██▊       | 28/100 [00:15<00:39,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:39,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:39,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:39,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.81it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.81it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.81it/s]

 29%|██▉       | 29/100 [00:16<00:39,  1.81it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.81it/s][A[A 29%|██▉       | 29/100 [00:16<00:39,  1.81it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.81it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.81it/s] 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 30%|███       | 30/100 [00:16<00:38,  1.80it/s]

 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 30%|███       | 30/100 [00:17<00:38,  1.80it/s][A[A 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 30%|███       | 30/100 [00:17<00:38,  1.80it/s] 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s]

 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s][A[A 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 31%|███       | 31/100 [00:17<00:38,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s]

 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s][A[A 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s]

 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s][A[A 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s]

 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s][A[A 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.80it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s]

 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s][A[A 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s]

 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s][A[A 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s]

 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s][A[A 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s]

 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s][A[A 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s]

 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s][A[A 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s]

 40%|████      | 40/100 [00:22<00:33,  1.79it/s][A[A 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s]

 41%|████      | 41/100 [00:23<00:32,  1.80it/s][A[A 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s]

 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s][A[A 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.81it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.81it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.81it/s]

 43%|████▎     | 43/100 [00:24<00:31,  1.81it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.81it/s][A[A 43%|████▎     | 43/100 [00:24<00:31,  1.81it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.81it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.81it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.81it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.81it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.81it/s]

 44%|████▍     | 44/100 [00:24<00:30,  1.81it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.81it/s][A[A 44%|████▍     | 44/100 [00:24<00:30,  1.81it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.81it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s]

 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s][A[A 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s]

 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s][A[A 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.81it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.81it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.81it/s]

 47%|████▋     | 47/100 [00:26<00:29,  1.81it/s][A[A 47%|████▋     | 47/100 [00:26<00:29,  1.81it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.81it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.81it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.81it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.81it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.81it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.81it/s]

 48%|████▊     | 48/100 [00:26<00:28,  1.81it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.81it/s][A[A 48%|████▊     | 48/100 [00:26<00:28,  1.81it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.81it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.81it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s]

 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s][A[A 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.82it/s] 50%|█████     | 50/100 [00:28<00:27,  1.82it/s] 50%|█████     | 50/100 [00:28<00:27,  1.82it/s] 50%|█████     | 50/100 [00:28<00:27,  1.82it/s]

 50%|█████     | 50/100 [00:28<00:27,  1.82it/s][A[A 50%|█████     | 50/100 [00:28<00:27,  1.82it/s] 50%|█████     | 50/100 [00:27<00:27,  1.82it/s] 50%|█████     | 50/100 [00:27<00:27,  1.82it/s] 50%|█████     | 50/100 [00:28<00:27,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s]

 51%|█████     | 51/100 [00:28<00:26,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s][A[A 51%|█████     | 51/100 [00:28<00:26,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s] 51%|█████     | 51/100 [00:28<00:26,  1.82it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.82it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.82it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.82it/s]

 52%|█████▏    | 52/100 [00:29<00:26,  1.82it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.82it/s][A[A 52%|█████▏    | 52/100 [00:29<00:26,  1.82it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.82it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.83it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.83it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.83it/s]

 53%|█████▎    | 53/100 [00:29<00:25,  1.83it/s][A[A 53%|█████▎    | 53/100 [00:29<00:25,  1.83it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.83it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.83it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.83it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.83it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.83it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.83it/s]

 54%|█████▍    | 54/100 [00:30<00:25,  1.83it/s][A[A 54%|█████▍    | 54/100 [00:30<00:25,  1.83it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.83it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.83it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.83it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s]

 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s][A[A 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s]

 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s][A[A 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s]

 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s][A[A 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s]

 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s][A[A 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 59%|█████▉    | 59/100 [00:32<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:32<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s]

 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s][A[A 59%|█████▉    | 59/100 [00:32<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:32<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:32<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s]

 60%|██████    | 60/100 [00:33<00:22,  1.81it/s][A[A 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s]

 61%|██████    | 61/100 [00:34<00:21,  1.82it/s][A[A 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s]

 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s][A[A 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s]

 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s][A[A 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.80it/s]

 64%|██████▍   | 64/100 [00:35<00:19,  1.80it/s][A[A 64%|██████▍   | 64/100 [00:35<00:19,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.80it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s]

 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s][A[A 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s]

 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s][A[A 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s]

 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s][A[A 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.81it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s]

 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s][A[A 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.83it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.83it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.83it/s]

 69%|██████▉   | 69/100 [00:38<00:16,  1.83it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.83it/s][A[A 69%|██████▉   | 69/100 [00:38<00:16,  1.83it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.83it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.83it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s]

 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s][A[A 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:38<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s]

 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s][A[A 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s]

 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s][A[A 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s]

 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s][A[A 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s]

 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s][A[A 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s]

 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s][A[A 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s]

 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s][A[A 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.83it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.83it/s]

 77%|███████▋  | 77/100 [00:42<00:12,  1.83it/s][A[A 77%|███████▋  | 77/100 [00:42<00:12,  1.83it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.83it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.83it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.83it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.83it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s]

 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s][A[A 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 79%|███████▉  | 79/100 [00:43<00:11,  1.81it/s]

 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s][A[A 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:43<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:43<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:43<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:10,  1.82it/s] 80%|████████  | 80/100 [00:44<00:10,  1.82it/s]

 80%|████████  | 80/100 [00:44<00:10,  1.82it/s] 80%|████████  | 80/100 [00:44<00:10,  1.82it/s] 80%|████████  | 80/100 [00:44<00:10,  1.82it/s][A[A 80%|████████  | 80/100 [00:44<00:10,  1.82it/s] 80%|████████  | 80/100 [00:44<00:10,  1.82it/s] 80%|████████  | 80/100 [00:44<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s]

 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s][A[A 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s]

 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s][A[A 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s]

 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s][A[A 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s]

 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s][A[A 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s]

 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s][A[A 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s]

 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s][A[A 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s]

 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s][A[A 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s]

 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s][A[A 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s]

 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s][A[A 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s]

 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s][A[A 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:49<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s]

 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s][A[A 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s]

 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s][A[A 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s]

 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s][A[A 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s]

 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s][A[A 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s]

 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s][A[A 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s]

 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s][A[A 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s]

 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s][A[A 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.80it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.81it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.81it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.81it/s]

 98%|█████████▊| 98/100 [00:54<00:01,  1.81it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.81it/s][A[A 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.81it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.81it/s]

 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:54<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:54<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:54<00:00,  1.81it/s][A[A 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:54<00:00,  1.81it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]


100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s][A[A100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]

100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]

100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
eval result: 0.6
dropped layer 2's attn
11/15/2023 21:20:54 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:20:54 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1165.95it/s]
100%|██████████| 3/3 [00:00<00:00, 1191.79it/s]
11/15/2023 21:20:54 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:20:54 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1115.70it/s]
100%|██████████| 3/3 [00:00<00:00, 1129.32it/s]
11/15/2023 21:20:54 - INFO - datasets.builder - No config specified, defaulting to the single config: piqa/plain_text
11/15/2023 21:20:54 - INFO - datasets.info - Loading Dataset Infos from /home/lanxiang/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:20:54 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/15/2023 21:20:54 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
Running loglikelihood requests
11/15/2023 21:20:54 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
11/15/2023 21:20:54 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
Running loglikelihood requests


  0%|          | 0/3 [00:00<?, ?it/s][A[A100%|██████████| 3/3 [00:00<00:00, 1239.33it/s]
Running loglikelihood requests
Running loglikelihood requests
11/15/2023 21:20:54 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:20:54 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1297.61it/s]
100%|██████████| 3/3 [00:00<00:00, 548.08it/s]
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
Running loglikelihood requests
Running loglikelihood requests


  0%|          | 0/100 [00:00<?, ?it/s][A[A  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]11/15/2023 21:20:54 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1202.61it/s]
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:11,  1.38it/s]

  1%|          | 1/100 [00:00<01:07,  1.46it/s][A[A  1%|          | 1/100 [00:00<01:11,  1.39it/s]  1%|          | 1/100 [00:00<01:06,  1.50it/s]  1%|          | 1/100 [00:00<01:10,  1.40it/s]  1%|          | 1/100 [00:00<01:12,  1.37it/s]  1%|          | 1/100 [00:00<00:58,  1.69it/s]  1%|          | 1/100 [00:00<01:07,  1.46it/s]

  2%|▏         | 2/100 [00:01<01:00,  1.62it/s][A[A  2%|▏         | 2/100 [00:01<01:02,  1.58it/s]  2%|▏         | 2/100 [00:01<00:59,  1.64it/s]  2%|▏         | 2/100 [00:01<01:01,  1.58it/s]  2%|▏         | 2/100 [00:01<01:02,  1.58it/s]  2%|▏         | 2/100 [00:01<00:56,  1.73it/s]  2%|▏         | 2/100 [00:01<01:01,  1.59it/s]  2%|▏         | 2/100 [00:01<01:00,  1.63it/s]  3%|▎         | 3/100 [00:01<00:58,  1.66it/s]

  3%|▎         | 3/100 [00:01<00:57,  1.68it/s][A[A  3%|▎         | 3/100 [00:01<00:58,  1.65it/s]  3%|▎         | 3/100 [00:01<00:57,  1.69it/s]  3%|▎         | 3/100 [00:01<00:58,  1.66it/s]  3%|▎         | 3/100 [00:01<00:58,  1.65it/s]  3%|▎         | 3/100 [00:01<00:57,  1.68it/s]  3%|▎         | 3/100 [00:01<00:55,  1.73it/s]  4%|▍         | 4/100 [00:02<00:55,  1.72it/s]

  4%|▍         | 4/100 [00:02<00:56,  1.71it/s][A[A  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:55,  1.72it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:54,  1.75it/s]

  5%|▌         | 5/100 [00:02<00:55,  1.72it/s][A[A  5%|▌         | 5/100 [00:03<00:55,  1.71it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  5%|▌         | 5/100 [00:03<00:55,  1.71it/s]  5%|▌         | 5/100 [00:03<00:55,  1.71it/s]  5%|▌         | 5/100 [00:03<00:55,  1.71it/s]  5%|▌         | 5/100 [00:02<00:54,  1.74it/s]  6%|▌         | 6/100 [00:03<00:55,  1.70it/s]

  6%|▌         | 6/100 [00:03<00:54,  1.71it/s]  6%|▌         | 6/100 [00:03<00:54,  1.71it/s][A[A  6%|▌         | 6/100 [00:03<00:54,  1.73it/s]  6%|▌         | 6/100 [00:03<00:55,  1.71it/s]  6%|▌         | 6/100 [00:03<00:54,  1.71it/s]  6%|▌         | 6/100 [00:03<00:55,  1.70it/s]  6%|▌         | 6/100 [00:03<00:55,  1.70it/s]  7%|▋         | 7/100 [00:04<00:54,  1.72it/s]  7%|▋         | 7/100 [00:04<00:53,  1.72it/s]

  7%|▋         | 7/100 [00:04<00:53,  1.72it/s]  7%|▋         | 7/100 [00:04<00:54,  1.72it/s][A[A  7%|▋         | 7/100 [00:04<00:53,  1.73it/s]  7%|▋         | 7/100 [00:04<00:54,  1.72it/s]  7%|▋         | 7/100 [00:04<00:54,  1.72it/s]  7%|▋         | 7/100 [00:04<00:54,  1.72it/s]  8%|▊         | 8/100 [00:04<00:52,  1.74it/s]  8%|▊         | 8/100 [00:04<00:52,  1.74it/s]  8%|▊         | 8/100 [00:04<00:52,  1.74it/s]

  8%|▊         | 8/100 [00:04<00:52,  1.74it/s]  8%|▊         | 8/100 [00:04<00:52,  1.74it/s][A[A  8%|▊         | 8/100 [00:04<00:52,  1.75it/s]  8%|▊         | 8/100 [00:04<00:52,  1.74it/s]  8%|▊         | 8/100 [00:04<00:52,  1.74it/s]  9%|▉         | 9/100 [00:05<00:51,  1.75it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]

  9%|▉         | 9/100 [00:05<00:51,  1.76it/s][A[A  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s]

 10%|█         | 10/100 [00:05<00:51,  1.74it/s][A[A 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s]

 11%|█         | 11/100 [00:06<00:50,  1.76it/s][A[A 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.77it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.76it/s]

 12%|█▏        | 12/100 [00:06<00:49,  1.77it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.77it/s][A[A 12%|█▏        | 12/100 [00:06<00:49,  1.77it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s]

 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s][A[A 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s]

 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s][A[A 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.76it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.76it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.76it/s]

 15%|█▌        | 15/100 [00:08<00:48,  1.76it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.76it/s][A[A 15%|█▌        | 15/100 [00:08<00:48,  1.76it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.76it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.76it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s]

 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s][A[A 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s]

 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s][A[A 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.79it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.79it/s]

 18%|█▊        | 18/100 [00:10<00:45,  1.79it/s][A[A 18%|█▊        | 18/100 [00:10<00:45,  1.79it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.79it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.79it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.79it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s]

 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s][A[A 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s]

 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s][A[A 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:45,  1.77it/s] 21%|██        | 21/100 [00:12<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.78it/s]

 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:12<00:44,  1.78it/s] 21%|██        | 21/100 [00:12<00:44,  1.78it/s][A[A 21%|██        | 21/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:12<00:44,  1.78it/s] 21%|██        | 21/100 [00:12<00:44,  1.78it/s]

 22%|██▏       | 22/100 [00:12<00:43,  1.77it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.77it/s][A[A 22%|██▏       | 22/100 [00:12<00:43,  1.77it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.77it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.77it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.77it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.77it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.77it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s]

 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s][A[A 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.78it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.78it/s]

 24%|██▍       | 24/100 [00:13<00:42,  1.78it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.78it/s][A[A 24%|██▍       | 24/100 [00:13<00:42,  1.78it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.78it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.78it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.78it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s]

 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s][A[A 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s]

 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s][A[A 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s]

 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s][A[A 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s]

 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s][A[A 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s]

 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s][A[A 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 30%|███       | 30/100 [00:17<00:38,  1.80it/s] 30%|███       | 30/100 [00:17<00:38,  1.80it/s] 30%|███       | 30/100 [00:17<00:38,  1.80it/s]

 30%|███       | 30/100 [00:17<00:38,  1.80it/s] 30%|███       | 30/100 [00:17<00:38,  1.80it/s][A[A 30%|███       | 30/100 [00:17<00:38,  1.80it/s] 30%|███       | 30/100 [00:17<00:38,  1.80it/s] 30%|███       | 30/100 [00:16<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s]

 31%|███       | 31/100 [00:17<00:38,  1.80it/s][A[A 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 31%|███       | 31/100 [00:17<00:38,  1.80it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s]

 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s][A[A 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s]

 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s][A[A 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.79it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s]

 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s][A[A 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s]

 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s][A[A 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s]

 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s][A[A 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s]

 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s][A[A 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.80it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s]

 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s][A[A 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.78it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s]

 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s][A[A 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s]

 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s][A[A 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 40%|████      | 40/100 [00:22<00:33,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s]

 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s][A[A 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s]

 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s][A[A 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s]

 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s][A[A 43%|████▎     | 43/100 [00:24<00:31,  1.80it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s]

 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s][A[A 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s]

 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s][A[A 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s]

 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s][A[A 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s]

 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s][A[A 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s]

 48%|████▊     | 48/100 [00:27<00:28,  1.82it/s][A[A 48%|████▊     | 48/100 [00:27<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s]

 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s][A[A 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s]

 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s][A[A 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s]

 51%|█████     | 51/100 [00:28<00:27,  1.81it/s][A[A 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s]

 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s][A[A 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s]

 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s][A[A 53%|█████▎    | 53/100 [00:29<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.80it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.80it/s]

 54%|█████▍    | 54/100 [00:30<00:25,  1.80it/s][A[A 54%|█████▍    | 54/100 [00:30<00:25,  1.80it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.80it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.80it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.80it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.80it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.81it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.81it/s]

 55%|█████▌    | 55/100 [00:30<00:24,  1.81it/s][A[A 55%|█████▌    | 55/100 [00:30<00:24,  1.81it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.81it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.81it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.81it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.81it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s]

 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s][A[A 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.81it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s]

 57%|█████▋    | 57/100 [00:32<00:23,  1.81it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s][A[A 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.81it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s]

 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s][A[A 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s]

 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s][A[A 59%|█████▉    | 59/100 [00:32<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s]

 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s][A[A 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s]

 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s][A[A 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s]

 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s][A[A 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s]

 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s][A[A 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.83it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.83it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.83it/s]

 64%|██████▍   | 64/100 [00:35<00:19,  1.83it/s][A[A 64%|██████▍   | 64/100 [00:35<00:19,  1.83it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.83it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.83it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s]

 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s][A[A 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s]

 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s][A[A 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.81it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s]

 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s][A[A 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s]

 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s][A[A 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.82it/s]

 69%|██████▉   | 69/100 [00:38<00:16,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.82it/s][A[A 69%|██████▉   | 69/100 [00:38<00:16,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s]

 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s][A[A 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.82it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s]

 71%|███████   | 71/100 [00:39<00:16,  1.81it/s][A[A 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s]

 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s][A[A 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:15,  1.79it/s] 73%|███████▎  | 73/100 [00:40<00:15,  1.79it/s]

 73%|███████▎  | 73/100 [00:40<00:15,  1.79it/s][A[A 73%|███████▎  | 73/100 [00:40<00:15,  1.79it/s] 73%|███████▎  | 73/100 [00:40<00:15,  1.79it/s] 73%|███████▎  | 73/100 [00:40<00:15,  1.79it/s] 73%|███████▎  | 73/100 [00:40<00:15,  1.79it/s] 73%|███████▎  | 73/100 [00:40<00:15,  1.79it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s]

 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s][A[A 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.81it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.81it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.81it/s]

 75%|███████▌  | 75/100 [00:41<00:13,  1.81it/s][A[A 75%|███████▌  | 75/100 [00:41<00:13,  1.81it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.81it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.81it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.81it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s]

 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s][A[A 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s]

 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s][A[A 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s]

 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s][A[A 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.82it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s]

 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s][A[A 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s] 79%|███████▉  | 79/100 [00:43<00:11,  1.83it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.83it/s] 80%|████████  | 80/100 [00:44<00:10,  1.83it/s] 80%|████████  | 80/100 [00:44<00:10,  1.83it/s] 80%|████████  | 80/100 [00:44<00:10,  1.83it/s]

 80%|████████  | 80/100 [00:44<00:10,  1.83it/s] 80%|████████  | 80/100 [00:44<00:10,  1.83it/s][A[A 80%|████████  | 80/100 [00:44<00:10,  1.83it/s] 80%|████████  | 80/100 [00:44<00:10,  1.83it/s] 80%|████████  | 80/100 [00:44<00:10,  1.83it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s]

 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s][A[A 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s]

 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s][A[A 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s]

 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s][A[A 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s]

 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s][A[A 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.83it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.83it/s]

 85%|████████▌ | 85/100 [00:47<00:08,  1.83it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.83it/s][A[A 85%|████████▌ | 85/100 [00:47<00:08,  1.83it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.83it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.83it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.83it/s]

 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s][A[A 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s]

 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s][A[A 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s]

 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s][A[A 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s]

 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s][A[A 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s]

 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s][A[A 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s]

 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s][A[A 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s]

 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s][A[A 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s]

 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s][A[A 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s]

 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s][A[A 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s]

 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s][A[A 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s]

 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s][A[A 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.83it/s]

 97%|█████████▋| 97/100 [00:53<00:01,  1.83it/s][A[A 97%|█████████▋| 97/100 [00:53<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.83it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.83it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s]

 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s][A[A 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s]

 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s][A[A 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:54<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s]

100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s][A[A100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.82it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]

100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
eval result: 0.62
dropped layer 2's ffn
11/15/2023 21:21:50 - INFO - datasets.builder - No config specified, defaulting to the single config: piqa/plain_text
11/15/2023 21:21:50 - INFO - datasets.info - Loading Dataset Infos from /home/lanxiang/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:21:50 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:21:50 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/15/2023 21:21:50 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
100%|██████████| 3/3 [00:00<00:00, 1144.32it/s]
11/15/2023 21:21:50 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:21:50 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
11/15/2023 21:21:50 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011


  0%|          | 0/3 [00:00<?, ?it/s][A[A100%|██████████| 3/3 [00:00<00:00, 1141.10it/s]
11/15/2023 21:21:50 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
100%|██████████| 3/3 [00:00<00:00, 1228.08it/s]
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1105.70it/s]
11/15/2023 21:21:50 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1273.06it/s]
Running loglikelihood requests
Running loglikelihood requests
Running loglikelihood requests
Running loglikelihood requests
11/15/2023 21:21:50 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
100%|██████████| 3/3 [00:00<00:00, 1154.50it/s]
11/15/2023 21:21:50 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s]

  0%|          | 0/100 [00:00<?, ?it/s][A[A100%|██████████| 3/3 [00:00<00:00, 1206.30it/s]
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]11/15/2023 21:21:50 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 956.73it/s]
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:14,  1.33it/s]

  1%|          | 1/100 [00:00<01:15,  1.31it/s][A[A  1%|          | 1/100 [00:00<01:10,  1.40it/s]  1%|          | 1/100 [00:00<01:10,  1.41it/s]  1%|          | 1/100 [00:00<00:58,  1.70it/s]  1%|          | 1/100 [00:00<01:16,  1.30it/s]  1%|          | 1/100 [00:00<01:16,  1.30it/s]  1%|          | 1/100 [00:00<01:15,  1.32it/s]

  2%|▏         | 2/100 [00:01<01:04,  1.53it/s][A[A  2%|▏         | 2/100 [00:01<01:03,  1.54it/s]  2%|▏         | 2/100 [00:01<01:01,  1.58it/s]  2%|▏         | 2/100 [00:01<00:56,  1.72it/s]  2%|▏         | 2/100 [00:01<01:02,  1.58it/s]  2%|▏         | 2/100 [00:01<01:04,  1.52it/s]  2%|▏         | 2/100 [00:01<01:04,  1.52it/s]  2%|▏         | 2/100 [00:01<01:03,  1.53it/s]  3%|▎         | 3/100 [00:01<00:58,  1.67it/s]

  3%|▎         | 3/100 [00:01<00:59,  1.63it/s][A[A  3%|▎         | 3/100 [00:01<00:59,  1.64it/s]  3%|▎         | 3/100 [00:01<00:55,  1.75it/s]  3%|▎         | 3/100 [00:01<00:58,  1.67it/s]  3%|▎         | 3/100 [00:01<00:59,  1.63it/s]  3%|▎         | 3/100 [00:01<00:59,  1.63it/s]  3%|▎         | 3/100 [00:01<00:59,  1.64it/s]

  4%|▍         | 4/100 [00:02<00:57,  1.68it/s][A[A  4%|▍         | 4/100 [00:02<00:57,  1.68it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:54,  1.75it/s]  4%|▍         | 4/100 [00:02<00:57,  1.67it/s]  4%|▍         | 4/100 [00:02<00:57,  1.68it/s]  4%|▍         | 4/100 [00:02<00:56,  1.70it/s]  4%|▍         | 4/100 [00:02<00:57,  1.68it/s]

  5%|▌         | 5/100 [00:03<00:55,  1.71it/s][A[A  5%|▌         | 5/100 [00:03<00:55,  1.72it/s]  5%|▌         | 5/100 [00:02<00:53,  1.76it/s]  5%|▌         | 5/100 [00:03<00:55,  1.71it/s]  5%|▌         | 5/100 [00:02<00:54,  1.73it/s]  5%|▌         | 5/100 [00:02<00:55,  1.73it/s]  5%|▌         | 5/100 [00:03<00:55,  1.72it/s]  5%|▌         | 5/100 [00:03<00:55,  1.71it/s]  6%|▌         | 6/100 [00:03<00:54,  1.71it/s]  6%|▌         | 6/100 [00:03<00:54,  1.74it/s]

  6%|▌         | 6/100 [00:03<00:54,  1.72it/s]  6%|▌         | 6/100 [00:03<00:55,  1.71it/s][A[A  6%|▌         | 6/100 [00:03<00:54,  1.71it/s]  6%|▌         | 6/100 [00:03<00:55,  1.71it/s]  6%|▌         | 6/100 [00:03<00:55,  1.71it/s]  6%|▌         | 6/100 [00:03<00:55,  1.71it/s]  7%|▋         | 7/100 [00:04<00:54,  1.72it/s]  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]

  7%|▋         | 7/100 [00:04<00:54,  1.72it/s][A[A  7%|▋         | 7/100 [00:04<00:53,  1.73it/s]  7%|▋         | 7/100 [00:04<00:54,  1.72it/s]  7%|▋         | 7/100 [00:04<00:53,  1.73it/s]  7%|▋         | 7/100 [00:04<00:54,  1.72it/s]  7%|▋         | 7/100 [00:04<00:54,  1.72it/s]  8%|▊         | 8/100 [00:04<00:52,  1.74it/s]  8%|▊         | 8/100 [00:04<00:53,  1.73it/s]  8%|▊         | 8/100 [00:04<00:53,  1.73it/s]

  8%|▊         | 8/100 [00:04<00:53,  1.73it/s][A[A  8%|▊         | 8/100 [00:04<00:53,  1.73it/s]  8%|▊         | 8/100 [00:04<00:53,  1.73it/s]  8%|▊         | 8/100 [00:04<00:53,  1.73it/s]  8%|▊         | 8/100 [00:04<00:53,  1.72it/s]  9%|▉         | 9/100 [00:05<00:52,  1.74it/s]  9%|▉         | 9/100 [00:05<00:51,  1.75it/s]  9%|▉         | 9/100 [00:05<00:52,  1.75it/s]  9%|▉         | 9/100 [00:05<00:52,  1.74it/s]

  9%|▉         | 9/100 [00:05<00:52,  1.74it/s][A[A  9%|▉         | 9/100 [00:05<00:52,  1.74it/s]  9%|▉         | 9/100 [00:05<00:52,  1.74it/s]  9%|▉         | 9/100 [00:05<00:52,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s]

 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s][A[A 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 10%|█         | 10/100 [00:05<00:51,  1.74it/s] 11%|█         | 11/100 [00:06<00:51,  1.74it/s] 11%|█         | 11/100 [00:06<00:51,  1.74it/s] 11%|█         | 11/100 [00:06<00:51,  1.74it/s]

 11%|█         | 11/100 [00:06<00:51,  1.74it/s][A[A 11%|█         | 11/100 [00:06<00:51,  1.74it/s] 11%|█         | 11/100 [00:06<00:51,  1.74it/s] 11%|█         | 11/100 [00:06<00:51,  1.74it/s] 11%|█         | 11/100 [00:06<00:51,  1.74it/s] 12%|█▏        | 12/100 [00:07<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s]

 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:07<00:50,  1.75it/s][A[A 12%|█▏        | 12/100 [00:07<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:07<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:07<00:50,  1.75it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.75it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.75it/s]

 13%|█▎        | 13/100 [00:07<00:49,  1.75it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.75it/s][A[A 13%|█▎        | 13/100 [00:07<00:49,  1.75it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.75it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.75it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.75it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s]

 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s][A[A 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s]

 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s][A[A 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s]

 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s][A[A 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s]

 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s][A[A 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.79it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.79it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.78it/s]

 18%|█▊        | 18/100 [00:10<00:45,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.78it/s][A[A 18%|█▊        | 18/100 [00:10<00:45,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s]

 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s][A[A 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.80it/s] 20%|██        | 20/100 [00:11<00:44,  1.80it/s]

 20%|██        | 20/100 [00:11<00:44,  1.80it/s] 20%|██        | 20/100 [00:11<00:44,  1.80it/s][A[A 20%|██        | 20/100 [00:11<00:44,  1.80it/s] 20%|██        | 20/100 [00:11<00:44,  1.80it/s] 20%|██        | 20/100 [00:11<00:44,  1.80it/s] 20%|██        | 20/100 [00:11<00:44,  1.80it/s] 21%|██        | 21/100 [00:12<00:43,  1.80it/s] 21%|██        | 21/100 [00:11<00:43,  1.80it/s]

 21%|██        | 21/100 [00:12<00:43,  1.80it/s] 21%|██        | 21/100 [00:12<00:43,  1.80it/s][A[A 21%|██        | 21/100 [00:12<00:43,  1.80it/s] 21%|██        | 21/100 [00:12<00:43,  1.80it/s] 21%|██        | 21/100 [00:12<00:43,  1.80it/s] 21%|██        | 21/100 [00:12<00:43,  1.80it/s]

 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s][A[A 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.80it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s]

 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s][A[A 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s]

 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s][A[A 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s]

 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s][A[A 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s]

 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s][A[A 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.80it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s]

 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s][A[A 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:41,  1.78it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s]

 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s][A[A 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s]

 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s][A[A 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 30%|███       | 30/100 [00:17<00:39,  1.78it/s] 30%|███       | 30/100 [00:16<00:39,  1.78it/s]

 30%|███       | 30/100 [00:17<00:39,  1.78it/s][A[A 30%|███       | 30/100 [00:17<00:39,  1.78it/s] 30%|███       | 30/100 [00:17<00:39,  1.78it/s] 30%|███       | 30/100 [00:17<00:39,  1.78it/s] 30%|███       | 30/100 [00:17<00:39,  1.78it/s] 30%|███       | 30/100 [00:17<00:39,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.79it/s] 31%|███       | 31/100 [00:17<00:38,  1.79it/s] 31%|███       | 31/100 [00:17<00:38,  1.79it/s]

 31%|███       | 31/100 [00:17<00:38,  1.79it/s][A[A 31%|███       | 31/100 [00:17<00:38,  1.79it/s] 31%|███       | 31/100 [00:17<00:38,  1.79it/s] 31%|███       | 31/100 [00:17<00:38,  1.79it/s] 31%|███       | 31/100 [00:17<00:38,  1.79it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s]

 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s][A[A 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.80it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s]

 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s][A[A 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.81it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.81it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.81it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.81it/s]

 34%|███▍      | 34/100 [00:19<00:36,  1.81it/s][A[A 34%|███▍      | 34/100 [00:19<00:36,  1.81it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.81it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.81it/s] 34%|███▍      | 34/100 [00:19<00:36,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s]

 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s][A[A 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 35%|███▌      | 35/100 [00:19<00:35,  1.81it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s]

 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s][A[A 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s]

 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s][A[A 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s]

 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s][A[A 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s]

 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s][A[A 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s]

 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s][A[A 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s]

 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s][A[A 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s]

 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s][A[A 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.80it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s]

 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s][A[A 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.79it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s]

 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s][A[A 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s]

 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s][A[A 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s]

 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s][A[A 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.81it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s]

 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s][A[A 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.80it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.80it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.80it/s]

 48%|████▊     | 48/100 [00:27<00:28,  1.80it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.80it/s][A[A 48%|████▊     | 48/100 [00:27<00:28,  1.80it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.80it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s]

 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s][A[A 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s]

 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s][A[A 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s]

 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s][A[A 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 51%|█████     | 51/100 [00:28<00:27,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s]

 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s][A[A 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s]

 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s][A[A 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s]

 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s][A[A 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s]

 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s][A[A 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s]

 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s][A[A 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s]

 57%|█████▋    | 57/100 [00:32<00:23,  1.83it/s][A[A 57%|█████▋    | 57/100 [00:31<00:23,  1.83it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.83it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.83it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.83it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.83it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.83it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.83it/s] 58%|█████▊    | 58/100 [00:32<00:22,  1.83it/s]

 58%|█████▊    | 58/100 [00:32<00:22,  1.83it/s] 58%|█████▊    | 58/100 [00:32<00:22,  1.83it/s][A[A 58%|█████▊    | 58/100 [00:32<00:22,  1.83it/s] 58%|█████▊    | 58/100 [00:32<00:22,  1.83it/s] 58%|█████▊    | 58/100 [00:32<00:22,  1.83it/s] 58%|█████▊    | 58/100 [00:32<00:22,  1.83it/s] 58%|█████▊    | 58/100 [00:32<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:32<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s]

 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s][A[A 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.83it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s]

 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s][A[A 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 60%|██████    | 60/100 [00:33<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s]

 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s][A[A 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s]

 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s][A[A 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s]

 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s][A[A 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s]

 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s][A[A 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.82it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s]

 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s][A[A 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.83it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.80it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.80it/s]

 66%|██████▌   | 66/100 [00:36<00:18,  1.80it/s][A[A 66%|██████▌   | 66/100 [00:36<00:18,  1.80it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.80it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.80it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.80it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s]

 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s][A[A 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.80it/s]

 68%|██████▊   | 68/100 [00:38<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.81it/s][A[A 68%|██████▊   | 68/100 [00:37<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s]

 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s][A[A 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s]

 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s][A[A 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 70%|███████   | 70/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s]

 71%|███████   | 71/100 [00:39<00:16,  1.80it/s][A[A 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s]

 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s][A[A 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s]

 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s][A[A 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s]

 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s][A[A 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s]

 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s][A[A 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s]

 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s][A[A 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.82it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.81it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.81it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.81it/s]

 77%|███████▋  | 77/100 [00:43<00:12,  1.81it/s][A[A 77%|███████▋  | 77/100 [00:42<00:12,  1.81it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.81it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.81it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s]

 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s][A[A 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.80it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:43<00:11,  1.81it/s]

 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s][A[A 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s]

 80%|████████  | 80/100 [00:44<00:11,  1.81it/s][A[A 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s]

 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s][A[A 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 81%|████████  | 81/100 [00:45<00:10,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s]

 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s][A[A 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.81it/s]

 83%|████████▎ | 83/100 [00:46<00:09,  1.81it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.81it/s][A[A 83%|████████▎ | 83/100 [00:46<00:09,  1.81it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.81it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.81it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.81it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s]

 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s][A[A 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s]

 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s][A[A 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.83it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.83it/s]

 86%|████████▌ | 86/100 [00:47<00:07,  1.83it/s][A[A 86%|████████▌ | 86/100 [00:47<00:07,  1.83it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.83it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.83it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s]

 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s][A[A 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.83it/s]

 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s][A[A 88%|████████▊ | 88/100 [00:48<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s]

 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s][A[A 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s]

 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s][A[A 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s]

 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s][A[A 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s]

 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s][A[A 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.81it/s]

 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s][A[A 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s]

 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s][A[A 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.82it/s]

 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s][A[A 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s]

 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s][A[A 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.83it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s]

 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s][A[A 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s]

 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s][A[A 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s]

 99%|█████████▉| 99/100 [00:54<00:00,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s][A[A 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.82it/s]100%|██████████| 100/100 [00:55<00:00,  1.83it/s]

100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.83it/s]
100%|██████████| 100/100 [00:55<00:00,  1.83it/s][A[A100%|██████████| 100/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.83it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]

100%|██████████| 100/100 [00:55<00:00,  1.80it/s]
eval result: 0.62
dropped layer 3's attn
11/15/2023 21:22:46 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1193.71it/s]
11/15/2023 21:22:46 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1119.08it/s]
11/15/2023 21:22:46 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:22:46 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1188.97it/s]
100%|██████████| 3/3 [00:00<00:00, 1151.23it/s]
11/15/2023 21:22:46 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:22:46 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1167.03it/s]
11/15/2023 21:22:46 - INFO - datasets.builder - No config specified, defaulting to the single config: piqa/plain_text
11/15/2023 21:22:46 - INFO - datasets.info - Loading Dataset Infos from /home/lanxiang/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
100%|██████████| 3/3 [00:00<00:00, 1206.88it/s]
11/15/2023 21:22:46 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/15/2023 21:22:46 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:22:46 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
11/15/2023 21:22:46 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011


  0%|          | 0/3 [00:00<?, ?it/s][A[A100%|██████████| 3/3 [00:00<00:00, 1293.47it/s]
Running loglikelihood requests
Running loglikelihood requests
Running loglikelihood requests
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
11/15/2023 21:22:46 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1230.84it/s]
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]

  0%|          | 0/100 [00:00<?, ?it/s][A[ARunning loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:58,  1.68it/s]

  1%|          | 1/100 [00:00<01:03,  1.57it/s][A[A  1%|          | 1/100 [00:00<01:03,  1.55it/s]  1%|          | 1/100 [00:00<01:08,  1.46it/s]  1%|          | 1/100 [00:00<01:07,  1.47it/s]  1%|          | 1/100 [00:00<01:06,  1.49it/s]  1%|          | 1/100 [00:00<01:07,  1.48it/s]  1%|          | 1/100 [00:00<01:04,  1.54it/s]

  2%|▏         | 2/100 [00:01<00:58,  1.67it/s][A[A  2%|▏         | 2/100 [00:01<00:57,  1.72it/s]  2%|▏         | 2/100 [00:01<01:00,  1.62it/s]  2%|▏         | 2/100 [00:01<01:00,  1.61it/s]  2%|▏         | 2/100 [00:01<00:59,  1.66it/s]  2%|▏         | 2/100 [00:01<00:59,  1.66it/s]  2%|▏         | 2/100 [00:01<01:00,  1.63it/s]  2%|▏         | 2/100 [00:01<01:00,  1.63it/s]  3%|▎         | 3/100 [00:01<00:56,  1.71it/s]

  3%|▎         | 3/100 [00:01<00:56,  1.72it/s][A[A  3%|▎         | 3/100 [00:01<00:55,  1.75it/s]  3%|▎         | 3/100 [00:01<00:57,  1.69it/s]  3%|▎         | 3/100 [00:01<00:57,  1.69it/s]  3%|▎         | 3/100 [00:01<00:56,  1.71it/s]  3%|▎         | 3/100 [00:01<00:57,  1.69it/s]  3%|▎         | 3/100 [00:01<00:57,  1.69it/s]

  4%|▍         | 4/100 [00:02<00:55,  1.73it/s][A[A  4%|▍         | 4/100 [00:02<00:55,  1.74it/s]  4%|▍         | 4/100 [00:02<00:56,  1.71it/s]  4%|▍         | 4/100 [00:02<00:56,  1.71it/s]  4%|▍         | 4/100 [00:02<00:56,  1.71it/s]  4%|▍         | 4/100 [00:02<00:55,  1.72it/s]  4%|▍         | 4/100 [00:02<00:55,  1.72it/s]  4%|▍         | 4/100 [00:02<00:56,  1.71it/s]

  5%|▌         | 5/100 [00:02<00:54,  1.73it/s][A[A  5%|▌         | 5/100 [00:02<00:54,  1.74it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  5%|▌         | 5/100 [00:02<00:54,  1.73it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  5%|▌         | 5/100 [00:02<00:55,  1.72it/s]  5%|▌         | 5/100 [00:02<00:54,  1.73it/s]  6%|▌         | 6/100 [00:03<00:53,  1.75it/s]  6%|▌         | 6/100 [00:03<00:54,  1.74it/s]  6%|▌         | 6/100 [00:03<00:54,  1.74it/s]

  6%|▌         | 6/100 [00:03<00:54,  1.74it/s]  6%|▌         | 6/100 [00:03<00:53,  1.74it/s][A[A  6%|▌         | 6/100 [00:03<00:54,  1.74it/s]  6%|▌         | 6/100 [00:03<00:53,  1.74it/s]  6%|▌         | 6/100 [00:03<00:53,  1.74it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]

  7%|▋         | 7/100 [00:04<00:53,  1.75it/s][A[A  7%|▋         | 7/100 [00:04<00:53,  1.74it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  7%|▋         | 7/100 [00:04<00:53,  1.75it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]

  8%|▊         | 8/100 [00:04<00:52,  1.76it/s][A[A  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.77it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]

  9%|▉         | 9/100 [00:05<00:51,  1.76it/s][A[A  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s]  9%|▉         | 9/100 [00:05<00:51,  1.76it/s] 10%|█         | 10/100 [00:05<00:50,  1.77it/s] 10%|█         | 10/100 [00:05<00:50,  1.77it/s] 10%|█         | 10/100 [00:05<00:50,  1.77it/s]

 10%|█         | 10/100 [00:05<00:50,  1.77it/s][A[A 10%|█         | 10/100 [00:05<00:50,  1.77it/s] 10%|█         | 10/100 [00:05<00:50,  1.77it/s] 10%|█         | 10/100 [00:05<00:50,  1.77it/s] 10%|█         | 10/100 [00:05<00:50,  1.77it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s]

 11%|█         | 11/100 [00:06<00:50,  1.75it/s][A[A 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s]

 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s][A[A 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.75it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s]

 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s][A[A 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.77it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s]

 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s][A[A 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.76it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.77it/s]

 15%|█▌        | 15/100 [00:08<00:48,  1.77it/s][A[A 15%|█▌        | 15/100 [00:08<00:48,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.78it/s]

 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s][A[A 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.78it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.77it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s]

 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s][A[A 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s]

 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s][A[A 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s]

 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s][A[A 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s]

 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s][A[A 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 20%|██        | 20/100 [00:11<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.76it/s] 21%|██        | 21/100 [00:12<00:44,  1.76it/s] 21%|██        | 21/100 [00:11<00:44,  1.76it/s]

 21%|██        | 21/100 [00:11<00:44,  1.76it/s][A[A 21%|██        | 21/100 [00:11<00:44,  1.76it/s] 21%|██        | 21/100 [00:11<00:44,  1.76it/s] 21%|██        | 21/100 [00:11<00:44,  1.76it/s] 21%|██        | 21/100 [00:11<00:44,  1.76it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s]

 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s][A[A 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s]

 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s][A[A 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 23%|██▎       | 23/100 [00:13<00:43,  1.78it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s]

 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s][A[A 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.77it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.77it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.77it/s]

 25%|██▌       | 25/100 [00:14<00:42,  1.77it/s][A[A 25%|██▌       | 25/100 [00:14<00:42,  1.77it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.77it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.77it/s] 25%|██▌       | 25/100 [00:14<00:42,  1.77it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.77it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.77it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.77it/s]

 26%|██▌       | 26/100 [00:14<00:41,  1.77it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.77it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.77it/s][A[A 26%|██▌       | 26/100 [00:14<00:41,  1.77it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.77it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s]

 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s][A[A 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s]

 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s][A[A 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s]

 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s][A[A 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 29%|██▉       | 29/100 [00:16<00:39,  1.80it/s] 30%|███       | 30/100 [00:16<00:39,  1.79it/s] 30%|███       | 30/100 [00:17<00:39,  1.79it/s] 30%|███       | 30/100 [00:16<00:39,  1.79it/s]

 30%|███       | 30/100 [00:16<00:39,  1.79it/s][A[A 30%|███       | 30/100 [00:17<00:39,  1.79it/s] 30%|███       | 30/100 [00:17<00:39,  1.79it/s] 30%|███       | 30/100 [00:17<00:39,  1.79it/s] 30%|███       | 30/100 [00:16<00:39,  1.79it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s]

 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s][A[A 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 32%|███▏      | 32/100 [00:18<00:38,  1.78it/s] 32%|███▏      | 32/100 [00:18<00:38,  1.78it/s]

 32%|███▏      | 32/100 [00:18<00:38,  1.78it/s] 32%|███▏      | 32/100 [00:18<00:38,  1.78it/s][A[A 32%|███▏      | 32/100 [00:18<00:38,  1.78it/s] 32%|███▏      | 32/100 [00:18<00:38,  1.78it/s] 32%|███▏      | 32/100 [00:18<00:38,  1.78it/s] 32%|███▏      | 32/100 [00:18<00:38,  1.78it/s]

 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s][A[A 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s]

 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s][A[A 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s]

 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s][A[A 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.78it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s]

 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s][A[A 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s]

 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s][A[A 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:35,  1.79it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.80it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.80it/s]

 38%|███▊      | 38/100 [00:21<00:34,  1.80it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.80it/s][A[A 38%|███▊      | 38/100 [00:21<00:34,  1.80it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.80it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.80it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s]

 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s][A[A 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.78it/s] 40%|████      | 40/100 [00:22<00:33,  1.78it/s] 40%|████      | 40/100 [00:22<00:33,  1.78it/s]

 40%|████      | 40/100 [00:22<00:33,  1.78it/s] 40%|████      | 40/100 [00:22<00:33,  1.78it/s] 40%|████      | 40/100 [00:22<00:33,  1.78it/s][A[A 40%|████      | 40/100 [00:22<00:33,  1.78it/s] 40%|████      | 40/100 [00:22<00:33,  1.78it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s]

 41%|████      | 41/100 [00:23<00:32,  1.79it/s][A[A 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 41%|████      | 41/100 [00:23<00:32,  1.79it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s]

 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s][A[A 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.79it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.80it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.80it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.80it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.80it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.80it/s]

 43%|████▎     | 43/100 [00:24<00:31,  1.80it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.80it/s][A[A 43%|████▎     | 43/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s]

 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s][A[A 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 44%|████▍     | 44/100 [00:24<00:31,  1.80it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s]

 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s][A[A 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.81it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s]

 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s][A[A 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 46%|████▌     | 46/100 [00:25<00:30,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s]

 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s][A[A 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.80it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.81it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.81it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.81it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.81it/s]

 48%|████▊     | 48/100 [00:27<00:28,  1.81it/s][A[A 48%|████▊     | 48/100 [00:27<00:28,  1.81it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.81it/s] 48%|████▊     | 48/100 [00:27<00:28,  1.81it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s]

 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s][A[A 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.80it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s]

 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s][A[A 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s]

 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s][A[A 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s]

 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s][A[A 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s]

 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s][A[A 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s]

 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s][A[A 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s]

 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s][A[A 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.81it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.81it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.81it/s]

 56%|█████▌    | 56/100 [00:31<00:24,  1.81it/s][A[A 56%|█████▌    | 56/100 [00:31<00:24,  1.81it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.81it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.81it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.81it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.82it/s]

 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s][A[A 57%|█████▋    | 57/100 [00:32<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:32<00:23,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s]

 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s][A[A 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s]

 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s][A[A 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.80it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s]

 60%|██████    | 60/100 [00:33<00:22,  1.80it/s][A[A 60%|██████    | 60/100 [00:33<00:22,  1.80it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.81it/s]

 61%|██████    | 61/100 [00:34<00:21,  1.80it/s] 61%|██████    | 61/100 [00:34<00:21,  1.80it/s][A[A 61%|██████    | 61/100 [00:34<00:21,  1.80it/s] 61%|██████    | 61/100 [00:34<00:21,  1.80it/s] 61%|██████    | 61/100 [00:34<00:21,  1.80it/s] 61%|██████    | 61/100 [00:34<00:21,  1.80it/s] 61%|██████    | 61/100 [00:34<00:21,  1.80it/s] 61%|██████    | 61/100 [00:34<00:21,  1.80it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s]

 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s][A[A 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.81it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.80it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.80it/s]

 63%|██████▎   | 63/100 [00:35<00:20,  1.80it/s][A[A 63%|██████▎   | 63/100 [00:35<00:20,  1.80it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.80it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.80it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.80it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.80it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s]

 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s][A[A 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s]

 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s][A[A 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.81it/s] 66%|██████▌   | 66/100 [00:37<00:18,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s]

 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s][A[A 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s]

 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s][A[A 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s]

 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s][A[A 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s]

 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s][A[A 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s]

 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s][A[A 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s]

 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s][A[A 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s]

 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s][A[A 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s]

 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s][A[A 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s]

 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s][A[A 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s]

 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s][A[A 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s]

 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s][A[A 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.81it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.80it/s]

 77%|███████▋  | 77/100 [00:43<00:12,  1.80it/s][A[A 77%|███████▋  | 77/100 [00:43<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:43<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s]

 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s][A[A 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s]

 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s][A[A 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.81it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s]

 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s][A[A 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 81%|████████  | 81/100 [00:45<00:10,  1.80it/s]

 81%|████████  | 81/100 [00:45<00:10,  1.80it/s] 81%|████████  | 81/100 [00:45<00:10,  1.80it/s] 81%|████████  | 81/100 [00:45<00:10,  1.80it/s][A[A 81%|████████  | 81/100 [00:45<00:10,  1.80it/s] 81%|████████  | 81/100 [00:45<00:10,  1.80it/s] 81%|████████  | 81/100 [00:45<00:10,  1.80it/s] 81%|████████  | 81/100 [00:45<00:10,  1.80it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s]

 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s][A[A 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.81it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s]

 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s][A[A 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.83it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.83it/s]

 84%|████████▍ | 84/100 [00:46<00:08,  1.83it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.83it/s][A[A 84%|████████▍ | 84/100 [00:46<00:08,  1.83it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.83it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.83it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.83it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s]

 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s][A[A 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.81it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s] 86%|████████▌ | 86/100 [00:48<00:07,  1.81it/s]

 86%|████████▌ | 86/100 [00:48<00:07,  1.81it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s] 86%|████████▌ | 86/100 [00:48<00:07,  1.81it/s][A[A 86%|████████▌ | 86/100 [00:47<00:07,  1.81it/s] 86%|████████▌ | 86/100 [00:48<00:07,  1.81it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s]

 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s][A[A 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s]

 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s][A[A 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s]

 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s][A[A 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.83it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.80it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.80it/s]

 90%|█████████ | 90/100 [00:50<00:05,  1.80it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.80it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.80it/s][A[A 90%|█████████ | 90/100 [00:50<00:05,  1.80it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.80it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.80it/s] 91%|█████████ | 91/100 [00:50<00:05,  1.80it/s]

 91%|█████████ | 91/100 [00:50<00:05,  1.80it/s] 91%|█████████ | 91/100 [00:50<00:05,  1.80it/s] 91%|█████████ | 91/100 [00:50<00:05,  1.80it/s][A[A 91%|█████████ | 91/100 [00:50<00:05,  1.80it/s] 91%|█████████ | 91/100 [00:50<00:05,  1.80it/s] 91%|█████████ | 91/100 [00:50<00:05,  1.80it/s] 91%|█████████ | 91/100 [00:50<00:05,  1.80it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.80it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.80it/s]

 92%|█████████▏| 92/100 [00:51<00:04,  1.80it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.80it/s][A[A 92%|█████████▏| 92/100 [00:51<00:04,  1.80it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.80it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.80it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s]

 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s][A[A 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.80it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s]

 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s][A[A 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.81it/s] 95%|█████████▌| 95/100 [00:53<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s]

 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s][A[A 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.81it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s]

 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s][A[A 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s]

 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s][A[A 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 97%|█████████▋| 97/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s]

 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s][A[A 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 98%|█████████▊| 98/100 [00:54<00:01,  1.82it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s]

 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s][A[A 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s] 99%|█████████▉| 99/100 [00:55<00:00,  1.83it/s]

100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.81it/s][A[A100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.80it/s]100%|██████████| 100/100 [00:55<00:00,  1.79it/s]100%|██████████| 100/100 [00:55<00:00,  1.79it/s]
100%|██████████| 100/100 [00:55<00:00,  1.80it/s]


100%|██████████| 100/100 [00:55<00:00,  1.81it/s]100%|██████████| 100/100 [00:55<00:00,  1.79it/s]
100%|██████████| 100/100 [00:55<00:00,  1.79it/s]
100%|██████████| 100/100 [00:55<00:00,  1.79it/s]
100%|██████████| 100/100 [00:55<00:00,  1.79it/s]
eval result: 0.42
dropped layer 3's ffn
11/15/2023 21:23:42 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:23:42 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1113.53it/s]
100%|██████████| 3/3 [00:00<00:00, 1185.72it/s]
11/15/2023 21:23:42 - INFO - datasets.builder - No config specified, defaulting to the single config: piqa/plain_text
11/15/2023 21:23:42 - INFO - datasets.info - Loading Dataset Infos from /home/lanxiang/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:23:42 - INFO - datasets.builder - Overwrite dataset info from restored data version.
11/15/2023 21:23:42 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011
11/15/2023 21:23:42 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
11/15/2023 21:23:42 - INFO - datasets.info - Loading Dataset info from /home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011


  0%|          | 0/3 [00:00<?, ?it/s][A[A11/15/2023 21:23:42 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:23:42 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1139.45it/s]
100%|██████████| 3/3 [00:00<00:00, 1142.24it/s]
100%|██████████| 3/3 [00:00<00:00, 1258.29it/s]
11/15/2023 21:23:42 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1263.73it/s]
Running loglikelihood requests
Running loglikelihood requests
11/15/2023 21:23:42 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]11/15/2023 21:23:42 - WARNING - datasets.builder - Found cached dataset piqa (/home/lanxiang/.cache/huggingface/datasets/piqa/plain_text/1.1.0/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 1165.30it/s]
100%|██████████| 3/3 [00:00<00:00, 1203.65it/s]
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
Running loglikelihood requests


  0%|          | 0/100 [00:00<?, ?it/s][A[A  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]Running loglikelihood requests
Running loglikelihood requests
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<01:02,  1.58it/s]

  1%|          | 1/100 [00:00<01:03,  1.56it/s][A[A  1%|          | 1/100 [00:00<01:03,  1.56it/s]  1%|          | 1/100 [00:00<01:03,  1.56it/s]  1%|          | 1/100 [00:00<01:05,  1.52it/s]  1%|          | 1/100 [00:00<01:00,  1.62it/s]  1%|          | 1/100 [00:00<01:01,  1.62it/s]  1%|          | 1/100 [00:00<01:05,  1.51it/s]

  2%|▏         | 2/100 [00:01<00:58,  1.68it/s][A[A  2%|▏         | 2/100 [00:01<00:58,  1.69it/s]  2%|▏         | 2/100 [00:01<00:58,  1.68it/s]  2%|▏         | 2/100 [00:01<00:59,  1.66it/s]  2%|▏         | 2/100 [00:01<00:58,  1.68it/s]  2%|▏         | 2/100 [00:01<00:57,  1.71it/s]  2%|▏         | 2/100 [00:01<00:57,  1.71it/s]  2%|▏         | 2/100 [00:01<00:59,  1.66it/s]

  3%|▎         | 3/100 [00:01<00:56,  1.72it/s][A[A  3%|▎         | 3/100 [00:01<00:56,  1.73it/s]  3%|▎         | 3/100 [00:01<00:56,  1.73it/s]  3%|▎         | 3/100 [00:01<00:56,  1.73it/s]  3%|▎         | 3/100 [00:01<00:56,  1.71it/s]  3%|▎         | 3/100 [00:01<00:56,  1.71it/s]  3%|▎         | 3/100 [00:01<00:55,  1.74it/s]  3%|▎         | 3/100 [00:01<00:55,  1.74it/s]

  4%|▍         | 4/100 [00:02<00:54,  1.75it/s][A[A  4%|▍         | 4/100 [00:02<00:54,  1.75it/s]  4%|▍         | 4/100 [00:02<00:55,  1.74it/s]  4%|▍         | 4/100 [00:02<00:54,  1.75it/s]  4%|▍         | 4/100 [00:02<00:54,  1.76it/s]  4%|▍         | 4/100 [00:02<00:55,  1.75it/s]  4%|▍         | 4/100 [00:02<00:55,  1.74it/s]  4%|▍         | 4/100 [00:02<00:54,  1.76it/s]

  5%|▌         | 5/100 [00:02<00:54,  1.75it/s][A[A  5%|▌         | 5/100 [00:02<00:54,  1.76it/s]  5%|▌         | 5/100 [00:02<00:54,  1.75it/s]  5%|▌         | 5/100 [00:02<00:54,  1.75it/s]  5%|▌         | 5/100 [00:02<00:54,  1.75it/s]  5%|▌         | 5/100 [00:02<00:53,  1.76it/s]  5%|▌         | 5/100 [00:02<00:54,  1.75it/s]  5%|▌         | 5/100 [00:02<00:53,  1.76it/s]  6%|▌         | 6/100 [00:03<00:53,  1.76it/s]  6%|▌         | 6/100 [00:03<00:53,  1.76it/s]  6%|▌         | 6/100 [00:03<00:53,  1.76it/s]  6%|▌         | 6/100 [00:03<00:53,  1.76it/s]

  6%|▌         | 6/100 [00:03<00:53,  1.76it/s][A[A  6%|▌         | 6/100 [00:03<00:53,  1.76it/s]  6%|▌         | 6/100 [00:03<00:53,  1.76it/s]  6%|▌         | 6/100 [00:03<00:53,  1.76it/s]  7%|▋         | 7/100 [00:04<00:52,  1.76it/s]  7%|▋         | 7/100 [00:04<00:52,  1.76it/s]

  7%|▋         | 7/100 [00:04<00:52,  1.76it/s][A[A  7%|▋         | 7/100 [00:04<00:52,  1.76it/s]  7%|▋         | 7/100 [00:04<00:52,  1.76it/s]  7%|▋         | 7/100 [00:04<00:52,  1.76it/s]  7%|▋         | 7/100 [00:04<00:52,  1.76it/s]  7%|▋         | 7/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]

  8%|▊         | 8/100 [00:04<00:52,  1.76it/s][A[A  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  8%|▊         | 8/100 [00:04<00:52,  1.76it/s]  9%|▉         | 9/100 [00:05<00:52,  1.75it/s]  9%|▉         | 9/100 [00:05<00:52,  1.75it/s]

  9%|▉         | 9/100 [00:05<00:52,  1.75it/s]  9%|▉         | 9/100 [00:05<00:52,  1.74it/s][A[A  9%|▉         | 9/100 [00:05<00:52,  1.75it/s]  9%|▉         | 9/100 [00:05<00:52,  1.74it/s]  9%|▉         | 9/100 [00:05<00:52,  1.75it/s]  9%|▉         | 9/100 [00:05<00:52,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s]

 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s][A[A 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 10%|█         | 10/100 [00:05<00:51,  1.75it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s]

 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s][A[A 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 11%|█         | 11/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s]

 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s][A[A 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 12%|█▏        | 12/100 [00:06<00:50,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s]

 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s][A[A 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 13%|█▎        | 13/100 [00:07<00:49,  1.76it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.77it/s]

 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s][A[A 14%|█▍        | 14/100 [00:08<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.77it/s] 14%|█▍        | 14/100 [00:07<00:48,  1.77it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.75it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.75it/s]

 15%|█▌        | 15/100 [00:08<00:48,  1.75it/s][A[A 15%|█▌        | 15/100 [00:08<00:48,  1.75it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.75it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.75it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.75it/s] 15%|█▌        | 15/100 [00:08<00:48,  1.75it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.76it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.76it/s]

 16%|█▌        | 16/100 [00:09<00:47,  1.76it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.76it/s][A[A 16%|█▌        | 16/100 [00:09<00:47,  1.76it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.76it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.76it/s] 16%|█▌        | 16/100 [00:09<00:47,  1.76it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s]

 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s][A[A 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 17%|█▋        | 17/100 [00:09<00:46,  1.77it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s]

 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s][A[A 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 18%|█▊        | 18/100 [00:10<00:46,  1.78it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s]

 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s][A[A 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 19%|█▉        | 19/100 [00:10<00:45,  1.77it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s]

 20%|██        | 20/100 [00:11<00:44,  1.78it/s][A[A 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 20%|██        | 20/100 [00:11<00:44,  1.78it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s]

 21%|██        | 21/100 [00:11<00:44,  1.79it/s][A[A 21%|██        | 21/100 [00:11<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s] 21%|██        | 21/100 [00:11<00:44,  1.79it/s]

 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s][A[A 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 22%|██▏       | 22/100 [00:12<00:43,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s]

 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s][A[A 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 23%|██▎       | 23/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s]

 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s][A[A 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 24%|██▍       | 24/100 [00:13<00:42,  1.79it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s]

 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s][A[A 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 25%|██▌       | 25/100 [00:14<00:41,  1.80it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s]

 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s][A[A 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 26%|██▌       | 26/100 [00:14<00:41,  1.79it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.78it/s]

 27%|██▋       | 27/100 [00:15<00:40,  1.78it/s][A[A 27%|██▋       | 27/100 [00:15<00:40,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.78it/s] 27%|██▋       | 27/100 [00:15<00:40,  1.78it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s]

 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s][A[A 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 28%|██▊       | 28/100 [00:15<00:40,  1.79it/s] 29%|██▉       | 29/100 [00:16<00:40,  1.77it/s] 29%|██▉       | 29/100 [00:16<00:40,  1.77it/s]

 29%|██▉       | 29/100 [00:16<00:40,  1.77it/s] 29%|██▉       | 29/100 [00:16<00:40,  1.77it/s][A[A 29%|██▉       | 29/100 [00:16<00:40,  1.77it/s] 29%|██▉       | 29/100 [00:16<00:40,  1.77it/s] 29%|██▉       | 29/100 [00:16<00:40,  1.77it/s] 29%|██▉       | 29/100 [00:16<00:40,  1.77it/s] 30%|███       | 30/100 [00:16<00:39,  1.79it/s] 30%|███       | 30/100 [00:16<00:39,  1.79it/s] 30%|███       | 30/100 [00:16<00:39,  1.79it/s] 30%|███       | 30/100 [00:16<00:39,  1.79it/s]

 30%|███       | 30/100 [00:16<00:39,  1.79it/s][A[A 30%|███       | 30/100 [00:16<00:39,  1.79it/s] 30%|███       | 30/100 [00:16<00:39,  1.79it/s] 30%|███       | 30/100 [00:16<00:39,  1.79it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s]

 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s][A[A 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 31%|███       | 31/100 [00:17<00:38,  1.78it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s]

 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s][A[A 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 32%|███▏      | 32/100 [00:18<00:37,  1.79it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s]

 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s][A[A 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 33%|███▎      | 33/100 [00:18<00:37,  1.77it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s]

 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s][A[A 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 34%|███▍      | 34/100 [00:19<00:37,  1.78it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s]

 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s][A[A 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 35%|███▌      | 35/100 [00:19<00:36,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s]

 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s][A[A 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 36%|███▌      | 36/100 [00:20<00:35,  1.79it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s]

 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s][A[A 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 37%|███▋      | 37/100 [00:20<00:34,  1.80it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s]

 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s][A[A 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 38%|███▊      | 38/100 [00:21<00:34,  1.81it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s]

 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s][A[A 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:22<00:33,  1.80it/s] 39%|███▉      | 39/100 [00:21<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s]

 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s][A[A 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s] 40%|████      | 40/100 [00:22<00:33,  1.80it/s]

 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 41%|████      | 41/100 [00:23<00:32,  1.81it/s][A[A 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 41%|████      | 41/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:31,  1.81it/s]

 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s][A[A 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 42%|████▏     | 42/100 [00:23<00:32,  1.81it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s]

 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s][A[A 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 43%|████▎     | 43/100 [00:24<00:31,  1.82it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s]

 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s][A[A 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 44%|████▍     | 44/100 [00:24<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s]

 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s][A[A 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 45%|████▌     | 45/100 [00:25<00:30,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s]

 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s][A[A 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 46%|████▌     | 46/100 [00:25<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s]

 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s][A[A 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 47%|████▋     | 47/100 [00:26<00:29,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s]

 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s][A[A 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 48%|████▊     | 48/100 [00:26<00:28,  1.82it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.78it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.78it/s]

 49%|████▉     | 49/100 [00:27<00:28,  1.78it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.78it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.78it/s][A[A 49%|████▉     | 49/100 [00:27<00:28,  1.78it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.78it/s] 49%|████▉     | 49/100 [00:27<00:28,  1.78it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s]

 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s][A[A 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 50%|█████     | 50/100 [00:28<00:27,  1.79it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s]

 51%|█████     | 51/100 [00:28<00:27,  1.80it/s][A[A 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 51%|█████     | 51/100 [00:28<00:27,  1.80it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s]

 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s][A[A 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 52%|█████▏    | 52/100 [00:29<00:26,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s]

 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s][A[A 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s] 53%|█████▎    | 53/100 [00:29<00:25,  1.81it/s]

 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s][A[A 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 54%|█████▍    | 54/100 [00:30<00:25,  1.81it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s]

 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s][A[A 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 55%|█████▌    | 55/100 [00:30<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s]

 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s][A[A 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 56%|█████▌    | 56/100 [00:31<00:24,  1.82it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s]

 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s][A[A 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s] 57%|█████▋    | 57/100 [00:31<00:23,  1.81it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s]

 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s][A[A 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 58%|█████▊    | 58/100 [00:32<00:23,  1.82it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s]

 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s][A[A 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 59%|█████▉    | 59/100 [00:33<00:22,  1.81it/s] 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:22,  1.82it/s]

 60%|██████    | 60/100 [00:33<00:22,  1.82it/s][A[A 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 60%|██████    | 60/100 [00:33<00:22,  1.82it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s]

 61%|██████    | 61/100 [00:34<00:21,  1.81it/s][A[A 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 61%|██████    | 61/100 [00:34<00:21,  1.81it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s]

 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s][A[A 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 62%|██████▏   | 62/100 [00:34<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s]

 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s][A[A 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.82it/s] 63%|██████▎   | 63/100 [00:35<00:20,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s]

 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s][A[A 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 64%|██████▍   | 64/100 [00:35<00:19,  1.81it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.82it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.82it/s]

 65%|██████▌   | 65/100 [00:36<00:19,  1.82it/s][A[A 65%|██████▌   | 65/100 [00:36<00:19,  1.82it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.82it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.82it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.82it/s] 65%|██████▌   | 65/100 [00:36<00:19,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s]

 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s][A[A 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 66%|██████▌   | 66/100 [00:36<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s]

 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s][A[A 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 67%|██████▋   | 67/100 [00:37<00:18,  1.82it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.81it/s]

 68%|██████▊   | 68/100 [00:37<00:17,  1.81it/s][A[A 68%|██████▊   | 68/100 [00:38<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:38<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.81it/s] 68%|██████▊   | 68/100 [00:37<00:17,  1.81it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s]

 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s][A[A 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 69%|██████▉   | 69/100 [00:38<00:17,  1.82it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s]

 70%|███████   | 70/100 [00:39<00:16,  1.80it/s][A[A 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 70%|███████   | 70/100 [00:39<00:16,  1.80it/s] 71%|███████   | 71/100 [00:39<00:15,  1.81it/s] 71%|███████   | 71/100 [00:39<00:15,  1.81it/s]

 71%|███████   | 71/100 [00:39<00:15,  1.81it/s] 71%|███████   | 71/100 [00:39<00:15,  1.81it/s] 71%|███████   | 71/100 [00:39<00:15,  1.81it/s][A[A 71%|███████   | 71/100 [00:39<00:15,  1.81it/s] 71%|███████   | 71/100 [00:39<00:15,  1.81it/s] 71%|███████   | 71/100 [00:39<00:16,  1.81it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s]

 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s][A[A 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s] 72%|███████▏  | 72/100 [00:40<00:15,  1.80it/s]

 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s][A[A 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.81it/s] 73%|███████▎  | 73/100 [00:40<00:14,  1.80it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s]

 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s][A[A 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 74%|███████▍  | 74/100 [00:41<00:14,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s]

 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s][A[A 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 75%|███████▌  | 75/100 [00:41<00:13,  1.82it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.83it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.83it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.83it/s]

 76%|███████▌  | 76/100 [00:42<00:13,  1.83it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.83it/s][A[A 76%|███████▌  | 76/100 [00:42<00:13,  1.83it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.83it/s] 76%|███████▌  | 76/100 [00:42<00:13,  1.83it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s]

 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s][A[A 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 77%|███████▋  | 77/100 [00:42<00:12,  1.80it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s]

 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s][A[A 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 78%|███████▊  | 78/100 [00:43<00:12,  1.81it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.79it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.79it/s]

 79%|███████▉  | 79/100 [00:44<00:11,  1.79it/s][A[A 79%|███████▉  | 79/100 [00:44<00:11,  1.79it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.79it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.79it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.79it/s] 79%|███████▉  | 79/100 [00:44<00:11,  1.79it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s]

 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s][A[A 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 80%|████████  | 80/100 [00:44<00:11,  1.80it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s]

 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s][A[A 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 81%|████████  | 81/100 [00:45<00:10,  1.81it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s]

 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s][A[A 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 82%|████████▏ | 82/100 [00:45<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.83it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s]

 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s][A[A 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.83it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.83it/s] 83%|████████▎ | 83/100 [00:46<00:09,  1.82it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s]

 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s][A[A 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 84%|████████▍ | 84/100 [00:46<00:08,  1.81it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s]

 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s][A[A 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 85%|████████▌ | 85/100 [00:47<00:08,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s]

 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s][A[A 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 86%|████████▌ | 86/100 [00:47<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s]

 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s][A[A 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 87%|████████▋ | 87/100 [00:48<00:07,  1.82it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s]

 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s][A[A 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:49<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s] 88%|████████▊ | 88/100 [00:48<00:06,  1.81it/s]

 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s][A[A 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.81it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.82it/s] 89%|████████▉ | 89/100 [00:49<00:06,  1.81it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s]

 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s][A[A 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 90%|█████████ | 90/100 [00:50<00:05,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s]

 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s][A[A 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s] 91%|█████████ | 91/100 [00:50<00:04,  1.82it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s]

 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s][A[A 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 92%|█████████▏| 92/100 [00:51<00:04,  1.83it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s]

 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s][A[A 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 93%|█████████▎| 93/100 [00:51<00:03,  1.82it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.83it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.83it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.83it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.83it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.83it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.83it/s] 94%|█████████▍| 94/100 [00:52<00:03,  1.83it/s]

 94%|█████████▍| 94/100 [00:52<00:03,  1.83it/s][A[A 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s]

 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s][A[A 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 95%|█████████▌| 95/100 [00:52<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s]

 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s][A[A 96%|█████████▌| 96/100 [00:53<00:02,  1.82it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.81it/s]

 97%|█████████▋| 97/100 [00:53<00:01,  1.81it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.81it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.81it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.81it/s][A[A 97%|█████████▋| 97/100 [00:53<00:01,  1.81it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.81it/s] 97%|█████████▋| 97/100 [00:53<00:01,  1.81it/s]